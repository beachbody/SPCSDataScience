[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to SPCSDataScience",
    "section": "",
    "text": "This repository is a series that will continue to evlove and find ways to use SPCS to make our lives easier as Data Scientist and Machine Learning Engineers. In this series we will be growing the snowflake native capabilities along side of SPCS to create a template to go in any direction that you are looking to be able to do.\nAfter you set up your work enviornment in snowflake the next steps are to build out the data science workflow you are going to go with. As this series develops",
    "crumbs": [
      "Welcome to SPCSDataScience"
    ]
  },
  {
    "objectID": "index.html#part-1-data-science-workload-in-snowflake-with-spcs",
    "href": "index.html#part-1-data-science-workload-in-snowflake-with-spcs",
    "title": "Welcome to SPCSDataScience",
    "section": "Part 1: Data Science Workload in Snowflake with SPCS",
    "text": "Part 1: Data Science Workload in Snowflake with SPCS\nYoutube Walk Through: Link\n\nMedium Article: Link\nGithub Page: Link",
    "crumbs": [
      "Welcome to SPCSDataScience"
    ]
  },
  {
    "objectID": "index.html#future-work",
    "href": "index.html#future-work",
    "title": "Welcome to SPCSDataScience",
    "section": "Future Work",
    "text": "Future Work\n\nData Ingestion\n\nData Ingestion Snowpark Great Article ideally this repo eventually becomes somehting like this that allows for you to have all of the data ingestion and ml ops with it as well.\n\nFeature Store & Data Preprocessing\n\nYes, snowflake is coming out with a feature store that is in PrPr, but coming soon we will go over what this might look like and how you can work through a probject with a feature store.\n\nOffline and batch feature stores will be\n\nData drift is part of both of these wanting to make sure that our data has not drifted to much because if it has then it might be time to retrain your model as your model’s enviornment has changed and you will not be getting the most out of your model’s prediction.\n\nModel Training and Model Registering\n\nThe goal here is to show the power of SPCS, Snowflake ML and Snowflake Cortext. As this is a work in progress this will take time to show all of these together, but the goal is to show how they can all work together as one.\n\nModel Inference\n\nShowing the options for model inference in snowflake.\n\n\nGithub Page – Currently not working yet",
    "crumbs": [
      "Welcome to SPCSDataScience"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html",
    "href": "jupyterlab_spcs.html",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "",
    "text": "You can go directly to this github and run this locally and follow along for the best results. If you run into errors please leave comments and I will do my best to get to them and make changes as necessary.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#what",
    "href": "jupyterlab_spcs.html#what",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "What?",
    "text": "What?\nSnowpark Container Services is a fully managed container offering that allows you to easily deploy, manage, and scale containerized services, jobs, and functions, all within the security and governance boundaries of Snowflake, and requiring zero data movement. As a fully managed service, SPCS comes with Snowflake’s native security, RBAC support, and built-in configuration and operational best-practices.\n\n\n\nSPCS\n\n\nIn this tutorial SPCS addresses a gap in Snowflake’s offerings a development space for data scientists that isn’t available out of the box. Although Snowflake notebooks are on the horizon, as seen here, they are not yet generally available. In the meantime, SPCS provides an effective solution that includes the ability to use git read/write capabilities. Snowflake recently introduced read-only Git integrations, but write capabilities are actively being developed. SPCS also allows you to bring your own libraries, including those not supported within the Snowflake ecosystem. By creating an image with the necessary tools and installing the Git integration of your choice, you can interact with your Git repository directly inside Snowflake. This capability is transformative for data scientists and machine learning engineers working securely within the Snowflake ecosystem.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#how",
    "href": "jupyterlab_spcs.html#how",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "How?",
    "text": "How?\nThrough the power of Snowpark Container Services (SPCS), we can now work on our preferred platform Snowflake while maintaining the flexibility we desire. This series will unveil the seemingly endless possibilities with Snowflake.\nOur Docker image will include persistent volumes that provide read and write access to internal stages within Snowflake. This setup allows you to log into your secure Snowflake ecosystem, pull your repository, and develop using the various compute layers offered by Snowflake. Whether your development requires large compute pools or GPUs, Snowflake’s elastic compute layer can accommodate these needs seamlessly and swiftly.\nNote: Many talented individuals and companies are already making significant advances with this technology. It’s important to clarify that this is just one approach to get started.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#why",
    "href": "jupyterlab_spcs.html#why",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Why?",
    "text": "Why?\nWith over 8 years of experience as a Data Scientist and Machine Learning Engineer, I’ve always sought to manage all our needs directly within Snowflake. Thanks to Snowpark Container Services (SPCS), this is now feasible without needing to switch platforms. My approach is greatly influenced by the notebook development methodology popularized by Fast.ai through nbdev, and you’ll see a Jupyter Notebook-first approach throughout this series.\nJeremy Howard has been an invaluable mentor to numerous prominent open-source developers, and I extend my gratitude to him for his contributions to the community. While you may not align with this development strategy, remember that it is flexible. If you prefer a different IDE, you can modify the Docker image to support Visual Studio Code and continue your work there. Initially, our setup will support Python in Jupyter Lab, but it is designed to be adaptable to other programming environments such as R Studio, Scala, or any other language that fits your development preferences.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#youtube-walk-through",
    "href": "jupyterlab_spcs.html#youtube-walk-through",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Youtube Walk Through",
    "text": "Youtube Walk Through",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#in-this-example",
    "href": "jupyterlab_spcs.html#in-this-example",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "In This Example:",
    "text": "In This Example:\nThis GitHub repository demonstrates a basic yet complete CI/CD process centered around Snowflake. The GitHub Action in this repo conducts both training and inference of a model sequentially. However, the idea is to schedule your inference or training with any tool. You can use the Snowflake Labs action here, but I had already developed the one used in this repo before its release—feel free to adjust as needed.\nIn future articles, we will explore Snowflake’s ML capabilities and add them to this series. Features will include native Snowflake preprocessing, a feature store, machine learning modeling, model registries, model management, Cortex baseline models, and more. Make sure to return to see how this series evolves.\n\n\n\n\n\n\nTask Example of Executing Docker Image\n\n\n\n\n\nHere is an example of what this would look like as a task.\n\ncreate or replace task DEMO_TASK_1\nUSER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\nSCHEDULE ='180 minutes'\nas\nEXECUTE SERVICE\n    IN COMPUTE POOL CONTAINER_DEMO_POOL\n    FROM SPECIFICATION $$\n    spec:\n      containers:\n        - name: modeling\n          image: sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo/modeling\n          env:\n            SNOWFLAKE_WAREHOUSE: CONTAINER_DEMO_WH\n            NOTEBOOK_FILE: \"nbs/99a_train_randomforest.ipynb\"\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - \"source /opt/venv/bin/activate && { datetime=$(date +'%Y%m%d_%H%M%S'); jupyter nbconvert --execute --inplace --allow-errors --ExecutePreprocessor.timeout=-1 --NotebookApp.token='${JUPYTER_TOKEN}' ${NOTEBOOK_FILE} --output=executed_${datetime}.ipynb; }\"\n          volumeMounts:\n            - name: juypter-nbs\n              mountPath: /home/jupyter\n      networkPolicyConfig:\n          allowInternetEgress: true\n      volumes:\n        - name: juypter-nbs\n          source: \"@volumes/juypter-nbs\"\n          uid: 1000\n          gid: 1000\n    \n    $$",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-1-create-a-role-with-required-privileges",
    "href": "jupyterlab_spcs.html#step-1-create-a-role-with-required-privileges",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 1: Create a Role with Required Privileges",
    "text": "Step 1: Create a Role with Required Privileges\nFirst, we initiate the process by switching to the ACCOUNTADMIN role to create a new role named CONTAINER_USER_ROLE. This role is then granted a set of privileges that are essential for the management and operation of databases, warehouses, and integrations within Snowflake. These privileges are crucial for creating and managing the infrastructure needed for containerized applications.\nUSE ROLE ACCOUNTADMIN;\nCREATE ROLE CONTAINER_USER_ROLE;\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\nGRANT CREATE COMPUTE POOL ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\nGRANT MONITOR USAGE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\nGRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\nGRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE CONTAINER_USER_ROLE;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-2-grant-the-new-role-to-accountadmin",
    "href": "jupyterlab_spcs.html#step-2-grant-the-new-role-to-accountadmin",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 2: Grant the New Role to ACCOUNTADMIN",
    "text": "Step 2: Grant the New Role to ACCOUNTADMIN\nAfter creating the CONTAINER_USER_ROLE, it’s necessary to grant this role to the ACCOUNTADMIN to ensure the administrator can utilize this new role. This step integrates the newly created role within the organizational roles hierarchy, allowing for seamless role management and access control.\nGRANT ROLE CONTAINER_USER_ROLE TO ROLE ACCOUNTADMIN;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-3-create-and-configure-a-warehouse",
    "href": "jupyterlab_spcs.html#step-3-create-and-configure-a-warehouse",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 3: Create and Configure a Warehouse",
    "text": "Step 3: Create and Configure a Warehouse\nNext, we focus on setting up a warehouse named CONTAINER_DEMO_WH with specified parameters to support the operational needs of container usage. This includes defining the warehouse size, auto-suspend, and auto-resume settings to optimize resource utilization.\nCREATE OR REPLACE WAREHOUSE CONTAINER_DEMO_WH\nWAREHOUSE_SIZE = XSMALL\nAUTO_SUSPEND = 120\nAUTO_RESUME = TRUE;\nGRANT OWNERSHIP ON WAREHOUSE CONTAINER_DEMO_WH TO ROLE CONTAINER_USER_ROLE REVOKE CURRENT GRANTS;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-4-setup-database-and-stages",
    "href": "jupyterlab_spcs.html#step-4-setup-database-and-stages",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 4: Setup Database and Stages",
    "text": "Step 4: Setup Database and Stages\nWith the CONTAINER_USER_ROLE, we proceed to create a database CONTAINER_DEMO_DB and configure stages for different purposes, including specs, volumes, and RDF models. Each stage is set up with encryption and directory enablement, tailored for specific use cases such as storing Docker image commands, data development, and modeling purposes.\nUSE ROLE CONTAINER_USER_ROLE;\nCREATE OR REPLACE DATABASE CONTAINER_DEMO_DB;\nUSE DATABASE CONTAINER_DEMO_DB;\n\nCREATE STAGE IF NOT EXISTS SPECS\nENCRYPTION = (TYPE='SNOWFLAKE_SSE')\nDIRECTORY = (ENABLE = TRUE)\nCOMMENT = 'Store Docker Image Commands for SPECS';\n\nCREATE STAGE IF NOT EXISTS VOLUMES\nENCRYPTION = (TYPE='SNOWFLAKE_SSE')\nDIRECTORY = (ENABLE = TRUE)\nCOMMENT = 'Saving Data As We Develop In Snowflake';\n\nCREATE STAGE IF NOT EXISTS RDF_MODEL\nENCRYPTION = (TYPE='SNOWFLAKE_SSE')\nDIRECTORY = (ENABLE = TRUE)\nCOMMENT = 'For This Use Case';",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-5-create-and-configure-image-repository",
    "href": "jupyterlab_spcs.html#step-5-create-and-configure-image-repository",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 5: Create and Configure Image Repository",
    "text": "Step 5: Create and Configure Image Repository\nFinally, an image repository named IMAGE_REPO is created within the CONTAINER_DEMO_DB.PUBLIC schema for modeling usage. This repository is essential for managing container images, facilitating the development and deployment process. Access rights are granted to the CONTAINER_USER_ROLE for both reading and writing, ensuring the role can fully utilize the repository for container management.\nCREATE IMAGE REPOSITORY CONTAINER_DEMO_DB.PUBLIC.IMAGE_REPO\nCOMMENT = 'This is for modeling usage';\n\nGRANT READ ON STAGE CONTAINER_DEMO_DB.PUBLIC.RDF_MODEL TO ROLE CONTAINER_USER_ROLE;\nGRANT WRITE ON STAGE CONTAINER_DEMO_DB.PUBLIC.RDF_MODEL TO ROLE CONTAINER_USER_ROLE;\n\nGRANT READ ON IMAGE REPOSITORY CONTAINER_DEMO_DB.PUBLIC.IMAGE_REPO TO ROLE CONTAINER_USER_ROLE;\nGRANT WRITE ON IMAGE REPOSITORY CONTAINER_DEMO_DB.PUBLIC.IMAGE_REPO TO ROLE CONTAINER_USER_ROLE;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#configure-yaml-files",
    "href": "jupyterlab_spcs.html#configure-yaml-files",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Configure Yaml Files",
    "text": "Configure Yaml Files\n\nWarning: Set up config.toml with snow sql need to be set up before running configure.sh and ensure Docker is on your computer\n\n\n# The last output of this will be the url you put here\n! rm ../modeling.yaml\n! rm ../Makefile\n! bash ../infra/configure.sh -r \"&lt;CHANGE_TO_YOUR_REPOSITORY_URL_HERE\" -w \"CONTAINER_DEMO_WH\" -i \"modeling\"\n\n\n! cat ../modeling.yaml\n\n\nMove Newly Created Spec File (modeling.yaml) to Stage @specs\n\nstage_location = r'@specs'\nstage_location = stage_location.strip('@')\nfile_path = '../modeling.yaml'\n\nput_results = session.file.put(local_file_name=file_path, stage_location=stage_location, auto_compress=False, overwrite=True)\nfor result in put_results:\n    print(f\"File: {result.source}, Status: {result.status}\")\ndisplay(pd.DataFrame(session.sql('ls @specs').collect()))",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#docker-creation",
    "href": "jupyterlab_spcs.html#docker-creation",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Docker Creation",
    "text": "Docker Creation\n\nRun Docker Build Locally\n\nWe want you to test this locally by designed to allow you to have a better understanding make build_local; make run and make sure it’s working as you would expect it then move on to the next steps.\n\n\nDownload docker desktop and ensure it is open and running.\n\n\nWarning: Make sure you have docker running and make sure you have logged in already ~/.docker/config.json you can check this or run docker login in the terminal\n\n\nAlso make sure you make your .env file in infra for your enviornment vairiables to be used in your docker-compose.yaml. As you grow out of this tutorial mode the next step here would be to create github actions to create these approaches so that you are able to run this in a more devops style approach. In future versions this will be shown.\n\n\nOpen Terminal navigate to this repo and run make build_local\n\nIf you are using VS code you can simple click terminal and paste the command\n\nAfter your local build is complete you can then run make run.\n\nThis is going to be running a docker compose process that can be viewed inside of the infra/ folder\n\n\n\nPictures of Steps\n\n\n\nMake build\n\n\n\n\n\nJupyterlab Initial\n\n\n\n\n\nJupyterlab Logged In\n\n\n\n\n\nJupyterlab Execute Imports\n\n\n\nNote: As SPCS will not allow your volumes to be respected in a two way methodology we have observed that COPY statements in your dockerfile which land in a directory which are then stage-mounted as a volume will NOT initialize the file in-stage. We have on open product gap for this but has thus far been low-priority currently so we are going to bring this repo to our development experience using the github, gitlab, bitbucket etc integrations later on in this process. So if you are wondering why we are using gh cli to bring the repo to the docker image this is one of the reasons.\n\n\nYou also need to make sure you are able to login in with snowql and snow cli to be able to run the following commands\n\n\n\n\nPushing Image To Snowflake\n\nmake all will –&gt; Login, Create, Push To Snowflake\n\n\n! cd ../ &&  make all\n\n\n\n\n\n\n\nExplain Make File Call\n\n\n\n\n\nI wanted to make sure that before we continue that we are taking a moment to understand the development life cyle of what we are building.\n\n\n\ndocker image life cycle\n\n\nWhen you run your make all command you are moving your docker image so that you are able to use this image as part of the service. This is a great picture below to show you what is actaully happening inside of snowflake with snowpark container services.\n\n\n\ndocker push\n\n\nThe make all command in the context of a Makefile is a target that typically encompasses a series of dependent tasks required to build a project. It’s designed to automate the process of compiling, building, and preparing a software application or service. Here’s a step-by-step explanation of what make all does in the provided Makefile script:\n\nlogin: This target logs into the Snowflake Docker repository. It’s the initial step to ensure that subsequent operations, such as pushing a Docker image, can authenticate against the Snowflake registry. The docker login command uses the $(SNOWFLAKE_REPO) variable, which should contain the URL of the Snowflake Docker repository.\nbuild: This target builds a Docker image for Snowpark Container Services. It specifies the platform as linux/amd64, uses a Dockerfile located in the infra directory, and tags the resulting image with the name specified in the $(DOCKER_IMAGE) variable. This step prepares the Docker image with the necessary environment and dependencies for the application.\npush_docker: After the Docker image is built, this target tags and pushes it to the Snowflake Container Services repository specified in the $(SNOWFLAKE_REPO) variable. This makes the Docker image available in Snowflake’s registry, allowing it to be used in Snowflake Container Services.\n\nIn summary, make all in this script is a composite command that automates the workflow of logging into the Snowflake Docker repository, building a Docker image tailored for Snowpark Container Services, and pushing the built image to the Snowflake repository. This streamlines the deployment process, ensuring that the Docker image is readily available in Snowflake for running services or applications.\n\n\n\n\nexecute_sql_file(\n    session,\n    file_path='DataScience/files/sql/01_container_services.sql'\n)\n\nChoosing your correct compute pools is going to be something that you will need to make sure you are choosing for you specific use case, but you can find all the compute pools currently available to each snowflake account. Compute Pools Engines are just like any of our other warehouses fully elastic and if you need more please reach out to snowflake to increase your quota.\n\n\n\n\n\n\nExplain Container Services Execution Code\n\n\n\n\n\nSnowflake Configuration for Enhanced Security and Services\nThis documentation guides you through the process of configuring Snowflake for enhanced security measures and service creation, specifically tailored for modeling and development purposes within containerized environments.\n\nStep 1: Create Security Integration for OAuth\nThe process begins with the creation of an OAuth security integration. This is a critical step for setting up authentication mechanisms, ensuring secure access to Snowflake services.\nUSE ROLE ACCOUNTADMIN;\nCREATE SECURITY INTEGRATION IF NOT EXISTS snowservices_ingress_oauth\n  TYPE=oauth\n  OAUTH_CLIENT=snowservices_ingress\n  ENABLED=true;\n\n\nStep 2: Configure Network Rules and External Access\nFollowing the security setup, network rules are defined to allow outbound traffic, facilitating external communications. An external access integration is also created to authorize this specified traffic, bolstering the security framework.\nCREATE OR REPLACE NETWORK RULE ALLOW_ALL_RULE\n  TYPE = 'HOST_PORT'\n  MODE = 'EGRESS'\n  VALUE_LIST= ('0.0.0.0:443', '0.0.0.0:80');\n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION ALLOW_ALL_EAI\n  ALLOWED_NETWORK_RULES = (ALLOW_ALL_RULE)\n  ENABLED = true;\n\n\nStep 3: Grant Usage on Integrations\nPermissions are then granted to the CONTAINER_USER_ROLE for the external access integration, ensuring the role has the necessary access to utilize the integration for service communications.\nGRANT USAGE ON INTEGRATION ALLOW_ALL_EAI TO ROLE CONTAINER_USER_ROLE;\n\n\nStep 4: Create and Modify Compute Pool\nA compute pool is established with specific node limits and an instance family, aligning with processing needs. This step also includes provisions for modifying the compute pool’s configuration to adjust resources as needed.\nCREATE COMPUTE POOL IF NOT EXISTS CONTAINER_DEMO_POOL\n  MIN_NODES = 1\n  MAX_NODES = 2\n  INSTANCE_FAMILY = standard_1;\nGRANT USAGE ON COMPUTE POOL CONTAINER_DEMO_POOL TO ROLE CONTAINER_USER_ROLE;\n\n\nStep 5: Create Modeling Snowpark Service\nNext, a Snowpark service named MODELING_SNOWPARK_SERVICE is created using the specified compute pool. This service is designed for modeling purposes, incorporating external access integrations and specification details for optimal functionality.\nUSE ROLE CONTAINER_USER_ROLE;\nCREATE SERVICE CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE\nin compute pool CONTAINER_DEMO_POOL\nfrom @SPECS\nspecification_file='modeling.yaml'\nexternal_access_integrations = (ALLOW_ALL_EAI)\nMIN_INSTANCES=1\nMAX_INSTANCES=1;\n\n\nStep 6: User Creation and Role Assignment\nFinally, a user is created with specified credentials, default role, and warehouse settings. The CONTAINER_USER_ROLE is granted to the user, along with usage permissions on various resources to ensure comprehensive access and operational capabilities.\nUSE ROLE ACCOUNTADMIN;\nCREATE OR REPLACE USER RANDOMEMPLOYEE\nIDENTIFIED BY 'Snowflake2024'\nDEFAULT_ROLE = 'CONTAINER_USER_ROLE'\nDEFAULT_WAREHOUSE = 'CONTAINER_DEMO_WH';\nGRANT ROLE CONTAINER_USER_ROLE TO USER RANDOMEMPLOYEE;\nGRANT USAGE ON WAREHOUSE CONTAINER_DEMO_WH TO ROLE CONTAINER_USER_ROLE;\nGRANT USAGE ON SERVICE CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE TO ROLE CONTAINER_USER_ROLE;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-1-create-security-integration-for-oauth",
    "href": "jupyterlab_spcs.html#step-1-create-security-integration-for-oauth",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 1: Create Security Integration for OAuth",
    "text": "Step 1: Create Security Integration for OAuth\nThe process begins with the creation of an OAuth security integration. This is a critical step for setting up authentication mechanisms, ensuring secure access to Snowflake services.\nUSE ROLE ACCOUNTADMIN;\nCREATE SECURITY INTEGRATION IF NOT EXISTS snowservices_ingress_oauth\n  TYPE=oauth\n  OAUTH_CLIENT=snowservices_ingress\n  ENABLED=true;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-2-configure-network-rules-and-external-access",
    "href": "jupyterlab_spcs.html#step-2-configure-network-rules-and-external-access",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 2: Configure Network Rules and External Access",
    "text": "Step 2: Configure Network Rules and External Access\nFollowing the security setup, network rules are defined to allow outbound traffic, facilitating external communications. An external access integration is also created to authorize this specified traffic, bolstering the security framework.\nCREATE OR REPLACE NETWORK RULE ALLOW_ALL_RULE\n  TYPE = 'HOST_PORT'\n  MODE = 'EGRESS'\n  VALUE_LIST= ('0.0.0.0:443', '0.0.0.0:80');\n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION ALLOW_ALL_EAI\n  ALLOWED_NETWORK_RULES = (ALLOW_ALL_RULE)\n  ENABLED = true;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-3-grant-usage-on-integrations",
    "href": "jupyterlab_spcs.html#step-3-grant-usage-on-integrations",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 3: Grant Usage on Integrations",
    "text": "Step 3: Grant Usage on Integrations\nPermissions are then granted to the CONTAINER_USER_ROLE for the external access integration, ensuring the role has the necessary access to utilize the integration for service communications.\nGRANT USAGE ON INTEGRATION ALLOW_ALL_EAI TO ROLE CONTAINER_USER_ROLE;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-4-create-and-modify-compute-pool",
    "href": "jupyterlab_spcs.html#step-4-create-and-modify-compute-pool",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 4: Create and Modify Compute Pool",
    "text": "Step 4: Create and Modify Compute Pool\nA compute pool is established with specific node limits and an instance family, aligning with processing needs. This step also includes provisions for modifying the compute pool’s configuration to adjust resources as needed.\nCREATE COMPUTE POOL IF NOT EXISTS CONTAINER_DEMO_POOL\n  MIN_NODES = 1\n  MAX_NODES = 2\n  INSTANCE_FAMILY = standard_1;\nGRANT USAGE ON COMPUTE POOL CONTAINER_DEMO_POOL TO ROLE CONTAINER_USER_ROLE;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-5-create-modeling-snowpark-service",
    "href": "jupyterlab_spcs.html#step-5-create-modeling-snowpark-service",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 5: Create Modeling Snowpark Service",
    "text": "Step 5: Create Modeling Snowpark Service\nNext, a Snowpark service named MODELING_SNOWPARK_SERVICE is created using the specified compute pool. This service is designed for modeling purposes, incorporating external access integrations and specification details for optimal functionality.\nUSE ROLE CONTAINER_USER_ROLE;\nCREATE SERVICE CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE\nin compute pool CONTAINER_DEMO_POOL\nfrom @SPECS\nspecification_file='modeling.yaml'\nexternal_access_integrations = (ALLOW_ALL_EAI)\nMIN_INSTANCES=1\nMAX_INSTANCES=1;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#step-6-user-creation-and-role-assignment",
    "href": "jupyterlab_spcs.html#step-6-user-creation-and-role-assignment",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Step 6: User Creation and Role Assignment",
    "text": "Step 6: User Creation and Role Assignment\nFinally, a user is created with specified credentials, default role, and warehouse settings. The CONTAINER_USER_ROLE is granted to the user, along with usage permissions on various resources to ensure comprehensive access and operational capabilities.\nUSE ROLE ACCOUNTADMIN;\nCREATE OR REPLACE USER RANDOMEMPLOYEE\nIDENTIFIED BY 'Snowflake2024'\nDEFAULT_ROLE = 'CONTAINER_USER_ROLE'\nDEFAULT_WAREHOUSE = 'CONTAINER_DEMO_WH';\nGRANT ROLE CONTAINER_USER_ROLE TO USER RANDOMEMPLOYEE;\nGRANT USAGE ON WAREHOUSE CONTAINER_DEMO_WH TO ROLE CONTAINER_USER_ROLE;\nGRANT USAGE ON SERVICE CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE TO ROLE CONTAINER_USER_ROLE;",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#look-into-service-created",
    "href": "jupyterlab_spcs.html#look-into-service-created",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Look Into Service Created",
    "text": "Look Into Service Created\n\nsession.sql(\"SHOW SERVICES;\").show()\n\nRemember: That this will take a few minutes of your service to be up and running so you will be able run the following command again and it will give you the url link that will allow you to log into your app with the correct Username and Password.\n\nCALL SYSTEM$GET_SERVICE_STATUS('CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE'); You can also try this command to see the status of your service you might get an ingress_url before the pending state is resolved.\n\n\ntemp = pd.DataFrame(session.sql(\"SHOW ENDPOINTS IN SERVICE MODELING_SNOWPARK_SERVICE;\").collect())\ndisplay(temp)\nprint(temp[\"ingress_url\"].values[0])\n\nAssuming ingest_url contains the URL you want to copy and paste this URL into your browser and enjoy your modeling jupyter notebook.\n\nOpen in web broswer and paste the ingress_url\nThis will open a login page for this demo it is\n\nUsename: RandomEmployee\nPassword: Snowflake2024\n\nDevelopment App Will Load and Your Application is ready to go",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#overview",
    "href": "jupyterlab_spcs.html#overview",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Overview",
    "text": "Overview\nThis guide will walk you through the process of working with the new Snowflake application. We will walk through how to log in and then cloning a repository, setting up GitHub secrets, triggering a GitHub Actions workflow, and exploring the resulting models and predictions stored in Snowflake.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#getting-started",
    "href": "jupyterlab_spcs.html#getting-started",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Getting Started",
    "text": "Getting Started\nWith the Docker image now deployed and ready, we are poised to explore the capabilities of our newly created application. Navigate to a website with a URL ending in .snowflakecomputing.app to begin.\n\n\n\nOpen Application\n\n\nUpon reaching the site, log in using the credentials for “RandomEmployee”. This process grants access to your development ecosystem.\n\nWarning: Allow 5-10 minutes for the application to become fully operational. Initial errors can often be resolved by refreshing the page or attempting to log in again after a brief wait.\n\nThe next step involves using a secure token found within your Docker Image. If unchanged, the default token “my_secure_token” enhances application security, although its use is optional.\n\n\n\nFirst Screen Add Token\n\n\nPost-login, you may notice the development ecosystem appears empty. This is due to the absence of a cloned repository. Although it may seem unconventional, cloning a repo into the development application addresses a known product gap, set for future resolution. Additionally, integration with Git in PrPr is forthcoming, offering a more streamlined experience with repositories.\n\n\n\nOpen Terminal as Jupyter Lab Is Empty\n\n\n\n\n\nClone Repo to Jupyter Lab Session\n\n\nThis will prompt your user name and password you can use a personal token to allow for yourself to load in your repo, but there are many other ways, but to keep it simple in this tutorial this was a good way to get this done quickly.\nHaving cloned your repository, feel free to start development. Remember to push changes back to your Git repository to remain updated with the latest revisions. Your code is safely stored in a designated volume, accessible via ls @volumes/jupyter-nbs/ in Snowflake, complying with specified requirements.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#github-integration",
    "href": "jupyterlab_spcs.html#github-integration",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "GitHub Integration",
    "text": "GitHub Integration\nTo integrate your project with GitHub and enable automated workflows, follow these steps:\n\nSet up GitHub Secrets: Ensure your GitHub repository contains the necessary secrets for authentication and access to your Snowflake instance. The setup should resemble the image below.\n\n\n\nAdd GitHub Secrets\n\n\nRefer to the GitHub Secrets documentation for detailed instructions on how to create and manage secrets for your repository.\nConfigure GitHub Actions: With GitHub secrets in place, navigate to your repository’s GitHub Actions to trigger the workflow.\n\n\n\nGo to Repo Actions\n\n\nTrigger the Workflow: Manually trigger the workflow by selecting the appropriate action and providing any necessary inputs.\n\n\n\nTrigger Workflow\n\n\nSuccessfully triggering the workflow culminates in the construction of a RandomForestRegressor. This model, once trained, is utilized for predictions, subsequently recorded back into Snowflake. Future tutorials will delve into more sophisticated workflows, enhancing your data science projects within Snowflake.\n\n\nCaution: This tutorial does not encompass the entirety of a data science project’s requirements. It serves as an introductory guide, with plans to expand on Snowflake’s native ML capabilities, Cortex, and other projects. Stay tuned for updates and explore different branches for additional developments.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#exploring-results",
    "href": "jupyterlab_spcs.html#exploring-results",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Exploring Results",
    "text": "Exploring Results\nAfter workflow completion, delve into the notebooks for insights and outcomes verification.\n\n\n\nClick into job_nbs\n\n\n\n\n\nClick into RanJobs\n\n\n\n\n\nResult of GitHub Actions\n\n\nReviewing these notebooks aids in debugging and ensures everything performed as expected.\n\n\n\nExplore Inference Notebook Run\n\n\nFor continued exploration, re-triggering the notebook facilitates additional predictions, exemplifying how automation, such as cron jobs in GitHub, can enhance your workflow.\n\n\n\nRun Action Again\n\n\nFinally, view the prediction outcomes within your Snowflake database.\n\n\n\nSnowflake Database",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#troubleshooting",
    "href": "jupyterlab_spcs.html#troubleshooting",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you encounter any issues during the deployment process, here are some common troubleshooting steps:\n\nLogin Issues: If you’re unable to log in, ensure that you’re using the correct credentials and that the application has had sufficient time to become operational.\nGitHub Secrets: Double-check that your GitHub secrets are set up correctly and that the required permissions are in place.\nWorkflow Errors: Review the output from the GitHub Actions workflow for any error messages or clues about what might be causing the issue.\nSnowflake Connectivity: Verify that your Snowflake instance is accessible and that you have the necessary permissions to interact with it.\n\nIf the issue persists, feel free to reach out to the support team for further assistance.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  },
  {
    "objectID": "jupyterlab_spcs.html#additional-resources",
    "href": "jupyterlab_spcs.html#additional-resources",
    "title": "JupyterLab In Snowflake With SPCS",
    "section": "Additional Resources",
    "text": "Additional Resources\nFor more advanced topics, code examples, and updates, please refer to the following resources:\n\nSnowflake Documentation\nSnowflake GitHub Repository\nSnowflake Community\nGitHub Actions Documentation\n\nWe encourage you to provide feedback on this documentation by submitting issues or pull requests.\n\nWARNING: This is intended for development purposes only and has not been approved by security or infosec for deployment. If you plan to deploy this, be aware of the risks, as it has not been fully secured. Please consult with your data governance teams to ensure safe deployment.",
    "crumbs": [
      "Tutorials",
      "JupyterLab In Snowflake With SPCS"
    ]
  }
]