{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f5eee2-6376-4b68-b501-3ca3385c6fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] ‘3.10.7.9000’"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(Robyn)\n",
    "packageVersion(\"Robyn\")\n",
    "Sys.setenv(R_FUTURE_FORK_ENABLE = \"true\")\n",
    "options(future.fork.enable = TRUE)\n",
    "create_files <- TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e278e22-1b33-496e-854a-de445b7652c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 12</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>DATE</th><th scope=col>revenue</th><th scope=col>tv_S</th><th scope=col>ooh_S</th><th scope=col>print_S</th><th scope=col>facebook_I</th><th scope=col>search_clicks_P</th><th scope=col>search_S</th><th scope=col>competitor_sales_B</th><th scope=col>facebook_S</th><th scope=col>events</th><th scope=col>newsletter</th></tr>\n",
       "\t<tr><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>2015-11-23</td><td>2754372</td><td>22358.35</td><td>     0.0</td><td>12728.4889</td><td>24301284</td><td>    0.000</td><td>   0.000</td><td>8125009</td><td>7607.1329</td><td>na</td><td>19401.65</td></tr>\n",
       "\t<tr><td>2015-11-30</td><td>2584277</td><td>28613.45</td><td>     0.0</td><td>    0.0000</td><td> 5527033</td><td> 9837.238</td><td>4133.333</td><td>7901549</td><td>1141.9525</td><td>na</td><td>14791.00</td></tr>\n",
       "\t<tr><td>2015-12-07</td><td>2547387</td><td>    0.00</td><td>132278.4</td><td>  453.8667</td><td>16651591</td><td>12044.120</td><td>3786.667</td><td>8300197</td><td>4256.3754</td><td>na</td><td>14544.00</td></tr>\n",
       "\t<tr><td>2015-12-14</td><td>2875220</td><td>83450.31</td><td>     0.0</td><td>17680.0000</td><td>10549766</td><td>12268.070</td><td>4253.333</td><td>8122883</td><td>2800.4907</td><td>na</td><td> 2800.00</td></tr>\n",
       "\t<tr><td>2015-12-21</td><td>2215953</td><td>    0.00</td><td>277336.0</td><td>    0.0000</td><td> 2934090</td><td> 9467.248</td><td>3613.333</td><td>7105985</td><td> 689.5826</td><td>na</td><td>15478.00</td></tr>\n",
       "\t<tr><td>2015-12-28</td><td>2569922</td><td>33225.31</td><td>     0.0</td><td>31922.3111</td><td>16634027</td><td>12687.259</td><td>3773.333</td><td>7097237</td><td>5337.0052</td><td>na</td><td>13817.00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 12\n",
       "\\begin{tabular}{llllllllllll}\n",
       " DATE & revenue & tv\\_S & ooh\\_S & print\\_S & facebook\\_I & search\\_clicks\\_P & search\\_S & competitor\\_sales\\_B & facebook\\_S & events & newsletter\\\\\n",
       " <date> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <dbl> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 2015-11-23 & 2754372 & 22358.35 &      0.0 & 12728.4889 & 24301284 &     0.000 &    0.000 & 8125009 & 7607.1329 & na & 19401.65\\\\\n",
       "\t 2015-11-30 & 2584277 & 28613.45 &      0.0 &     0.0000 &  5527033 &  9837.238 & 4133.333 & 7901549 & 1141.9525 & na & 14791.00\\\\\n",
       "\t 2015-12-07 & 2547387 &     0.00 & 132278.4 &   453.8667 & 16651591 & 12044.120 & 3786.667 & 8300197 & 4256.3754 & na & 14544.00\\\\\n",
       "\t 2015-12-14 & 2875220 & 83450.31 &      0.0 & 17680.0000 & 10549766 & 12268.070 & 4253.333 & 8122883 & 2800.4907 & na &  2800.00\\\\\n",
       "\t 2015-12-21 & 2215953 &     0.00 & 277336.0 &     0.0000 &  2934090 &  9467.248 & 3613.333 & 7105985 &  689.5826 & na & 15478.00\\\\\n",
       "\t 2015-12-28 & 2569922 & 33225.31 &      0.0 & 31922.3111 & 16634027 & 12687.259 & 3773.333 & 7097237 & 5337.0052 & na & 13817.00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 12\n",
       "\n",
       "| DATE &lt;date&gt; | revenue &lt;dbl&gt; | tv_S &lt;dbl&gt; | ooh_S &lt;dbl&gt; | print_S &lt;dbl&gt; | facebook_I &lt;dbl&gt; | search_clicks_P &lt;dbl&gt; | search_S &lt;dbl&gt; | competitor_sales_B &lt;int&gt; | facebook_S &lt;dbl&gt; | events &lt;chr&gt; | newsletter &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 2015-11-23 | 2754372 | 22358.35 |      0.0 | 12728.4889 | 24301284 |     0.000 |    0.000 | 8125009 | 7607.1329 | na | 19401.65 |\n",
       "| 2015-11-30 | 2584277 | 28613.45 |      0.0 |     0.0000 |  5527033 |  9837.238 | 4133.333 | 7901549 | 1141.9525 | na | 14791.00 |\n",
       "| 2015-12-07 | 2547387 |     0.00 | 132278.4 |   453.8667 | 16651591 | 12044.120 | 3786.667 | 8300197 | 4256.3754 | na | 14544.00 |\n",
       "| 2015-12-14 | 2875220 | 83450.31 |      0.0 | 17680.0000 | 10549766 | 12268.070 | 4253.333 | 8122883 | 2800.4907 | na |  2800.00 |\n",
       "| 2015-12-21 | 2215953 |     0.00 | 277336.0 |     0.0000 |  2934090 |  9467.248 | 3613.333 | 7105985 |  689.5826 | na | 15478.00 |\n",
       "| 2015-12-28 | 2569922 | 33225.31 |      0.0 | 31922.3111 | 16634027 | 12687.259 | 3773.333 | 7097237 | 5337.0052 | na | 13817.00 |\n",
       "\n"
      ],
      "text/plain": [
       "  DATE       revenue tv_S     ooh_S    print_S    facebook_I search_clicks_P\n",
       "1 2015-11-23 2754372 22358.35      0.0 12728.4889 24301284       0.000      \n",
       "2 2015-11-30 2584277 28613.45      0.0     0.0000  5527033    9837.238      \n",
       "3 2015-12-07 2547387     0.00 132278.4   453.8667 16651591   12044.120      \n",
       "4 2015-12-14 2875220 83450.31      0.0 17680.0000 10549766   12268.070      \n",
       "5 2015-12-21 2215953     0.00 277336.0     0.0000  2934090    9467.248      \n",
       "6 2015-12-28 2569922 33225.31      0.0 31922.3111 16634027   12687.259      \n",
       "  search_S competitor_sales_B facebook_S events newsletter\n",
       "1    0.000 8125009            7607.1329  na     19401.65  \n",
       "2 4133.333 7901549            1141.9525  na     14791.00  \n",
       "3 3786.667 8300197            4256.3754  na     14544.00  \n",
       "4 4253.333 8122883            2800.4907  na      2800.00  \n",
       "5 3613.333 7105985             689.5826  na     15478.00  \n",
       "6 3773.333 7097237            5337.0052  na     13817.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>ds</th><th scope=col>holiday</th><th scope=col>country</th><th scope=col>year</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1995-01-01</td><td>New Year's Day  </td><td>AD</td><td>1995</td></tr>\n",
       "\t<tr><td>1995-01-06</td><td>Epiphany        </td><td>AD</td><td>1995</td></tr>\n",
       "\t<tr><td>1995-02-28</td><td>Carnival        </td><td>AD</td><td>1995</td></tr>\n",
       "\t<tr><td>1995-03-14</td><td>Constitution Day</td><td>AD</td><td>1995</td></tr>\n",
       "\t<tr><td>1995-04-14</td><td>Good Friday     </td><td>AD</td><td>1995</td></tr>\n",
       "\t<tr><td>1995-04-17</td><td>Easter Monday   </td><td>AD</td><td>1995</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 4\n",
       "\\begin{tabular}{llll}\n",
       " ds & holiday & country & year\\\\\n",
       " <chr> & <chr> & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t 1995-01-01 & New Year's Day   & AD & 1995\\\\\n",
       "\t 1995-01-06 & Epiphany         & AD & 1995\\\\\n",
       "\t 1995-02-28 & Carnival         & AD & 1995\\\\\n",
       "\t 1995-03-14 & Constitution Day & AD & 1995\\\\\n",
       "\t 1995-04-14 & Good Friday      & AD & 1995\\\\\n",
       "\t 1995-04-17 & Easter Monday    & AD & 1995\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 4\n",
       "\n",
       "| ds &lt;chr&gt; | holiday &lt;chr&gt; | country &lt;chr&gt; | year &lt;int&gt; |\n",
       "|---|---|---|---|\n",
       "| 1995-01-01 | New Year's Day   | AD | 1995 |\n",
       "| 1995-01-06 | Epiphany         | AD | 1995 |\n",
       "| 1995-02-28 | Carnival         | AD | 1995 |\n",
       "| 1995-03-14 | Constitution Day | AD | 1995 |\n",
       "| 1995-04-14 | Good Friday      | AD | 1995 |\n",
       "| 1995-04-17 | Easter Monday    | AD | 1995 |\n",
       "\n"
      ],
      "text/plain": [
       "  ds         holiday          country year\n",
       "1 1995-01-01 New Year's Day   AD      1995\n",
       "2 1995-01-06 Epiphany         AD      1995\n",
       "3 1995-02-28 Carnival         AD      1995\n",
       "4 1995-03-14 Constitution Day AD      1995\n",
       "5 1995-04-14 Good Friday      AD      1995\n",
       "6 1995-04-17 Easter Monday    AD      1995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## IMPORTANT: Must install and setup the python library \"Nevergrad\" once before using Robyn\n",
    "## Guide: https://github.com/facebookexperimental/Robyn/blob/main/demo/install_nevergrad.R\n",
    "\n",
    "################################################################\n",
    "#### Step 1: Load data\n",
    "\n",
    "## Check simulated dataset or load your own dataset\n",
    "data(\"dt_simulated_weekly\")\n",
    "head(dt_simulated_weekly)\n",
    "\n",
    "## Check holidays from Prophet\n",
    "# 59 countries included. If your country is not included, please manually add it.\n",
    "# Tipp: any events can be added into this table, school break, events etc.\n",
    "data(\"dt_prophet_holidays\")\n",
    "head(dt_prophet_holidays)\n",
    "\n",
    "# Directory where you want to export results to (will create new folders)\n",
    "robyn_directory <- \"~/Desktop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273af682-744c-4ef0-bbaf-8f31e36943ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Step 2a: For first time user: Model specification in 4 steps\n",
    "\n",
    "#### 2a-1: First, specify input variables\n",
    "\n",
    "## All sign control are now automatically provided: \"positive\" for media & organic\n",
    "## variables and \"default\" for all others. User can still customise signs if necessary.\n",
    "## Documentation is available, access it anytime by running: ?robyn_inputs\n",
    "InputCollect <- robyn_inputs(\n",
    "  dt_input = dt_simulated_weekly,\n",
    "  dt_holidays = dt_prophet_holidays,\n",
    "  date_var = \"DATE\", # date format must be \"2020-01-01\"\n",
    "  dep_var = \"revenue\", # there should be only one dependent variable\n",
    "  dep_var_type = \"revenue\", # \"revenue\" (ROI) or \"conversion\" (CPA)\n",
    "  prophet_vars = c(\"trend\", \"season\", \"holiday\"), # \"trend\",\"season\", \"weekday\" & \"holiday\"\n",
    "  prophet_country = \"DE\", # input country code. Check: dt_prophet_holidays\n",
    "  context_vars = c(\"competitor_sales_B\", \"events\"), # e.g. competitors, discount, unemployment etc\n",
    "  paid_media_spends = c(\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"), # mandatory input\n",
    "  paid_media_vars = c(\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_I\", \"search_clicks_P\"), # mandatory.\n",
    "  # paid_media_vars must have same order as paid_media_spends. Use media exposure metrics like\n",
    "  # impressions, GRP etc. If not applicable, use spend instead.\n",
    "  organic_vars = \"newsletter\", # marketing activity without media spend\n",
    "  # factor_vars = c(\"events\"), # force variables in context_vars or organic_vars to be categorical\n",
    "  window_start = \"2016-01-01\",\n",
    "  window_end = \"2018-12-31\",\n",
    "  adstock = \"geometric\" # geometric, weibull_cdf or weibull_pdf.\n",
    ")\n",
    "print(InputCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2519dd-9bde-45e2-ad67-f8fa3d58ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2a-2: Second, define and add hyperparameters\n",
    "\n",
    "## Default media variable for modelling has changed from paid_media_vars to paid_media_spends.\n",
    "## Also, calibration_input are required to be spend names.\n",
    "## hyperparameter names are based on paid_media_spends names too. See right hyperparameter names:\n",
    "hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)\n",
    "\n",
    "## Guide to setup & understand hyperparameters\n",
    "\n",
    "## Robyn's hyperparameters have four components:\n",
    "## - Adstock parameters (theta or shape/scale)\n",
    "## - Saturation parameters (alpha/gamma)\n",
    "## - Regularisation parameter (lambda). No need to specify manually\n",
    "## - Time series validation parameter (train_size)\n",
    "\n",
    "## 1. IMPORTANT: set plot = TRUE to create example plots for adstock & saturation\n",
    "## hyperparameters and their influence in curve transformation.\n",
    "plot_adstock(plot = FALSE)\n",
    "plot_saturation(plot = FALSE)\n",
    "\n",
    "## 2. Get correct hyperparameter names:\n",
    "# All variables in paid_media_spends and organic_vars require hyperparameter and will be\n",
    "# transformed by adstock & saturation.\n",
    "# Run hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)\n",
    "# to get correct media hyperparameter names. All names in hyperparameters must equal\n",
    "# names from hyper_names(), case sensitive. Run ?hyper_names to check function arguments.\n",
    "\n",
    "## 3. Hyperparameter interpretation & recommendation:\n",
    "\n",
    "## Geometric adstock: Theta is the only parameter and means fixed decay rate. Assuming TV\n",
    "# spend on day 1 is 100€ and theta = 0.7, then day 2 has 100*0.7=70€ worth of effect\n",
    "# carried-over from day 1, day 3 has 70*0.7=49€ from day 2 etc. Rule-of-thumb for common\n",
    "# media genre: TV c(0.3, 0.8), OOH/Print/Radio c(0.1, 0.4), digital c(0, 0.3). Also,\n",
    "# to convert weekly to daily we can transform the parameter to the power of (1/7),\n",
    "# so to convert 30% daily to weekly is 0.3^(1/7) = 0.84.\n",
    "\n",
    "## Weibull CDF adstock: The Cumulative Distribution Function of Weibull has two parameters,\n",
    "# shape & scale, and has flexible decay rate, compared to Geometric adstock with fixed\n",
    "# decay rate. The shape parameter controls the shape of the decay curve. Recommended\n",
    "# bound is c(0, 2). The larger the shape, the more S-shape. The smaller, the more\n",
    "# L-shape. Scale controls the inflexion point of the decay curve. We recommend very\n",
    "# conservative bounce of c(0, 0.1), because scale increases the adstock half-life greatly.\n",
    "# When shape or scale is 0, adstock will be 0.\n",
    "\n",
    "## Weibull PDF adstock: The Probability Density Function of the Weibull also has two\n",
    "# parameters, shape & scale, and also has flexible decay rate as Weibull CDF. The\n",
    "# difference is that Weibull PDF offers lagged effect. When shape > 2, the curve peaks\n",
    "# after x = 0 and has NULL slope at x = 0, enabling lagged effect and sharper increase and\n",
    "# decrease of adstock, while the scale parameter indicates the limit of the relative\n",
    "# position of the peak at x axis; when 1 < shape < 2, the curve peaks after x = 0 and has\n",
    "# infinite positive slope at x = 0, enabling lagged effect and slower increase and decrease\n",
    "# of adstock, while scale has the same effect as above; when shape = 1, the curve peaks at\n",
    "# x = 0 and reduces to exponential decay, while scale controls the inflexion point; when\n",
    "# 0 < shape < 1, the curve peaks at x = 0 and has increasing decay, while scale controls\n",
    "# the inflexion point. When all possible shapes are relevant, we recommend c(0.0001, 10)\n",
    "# as bounds for shape; when only strong lagged effect is of interest, we recommend\n",
    "# c(2.0001, 10) as bound for shape. In all cases, we recommend conservative bound of\n",
    "# c(0, 0.1) for scale. Due to the great flexibility of Weibull PDF, meaning more freedom\n",
    "# in hyperparameter spaces for Nevergrad to explore, it also requires larger iterations\n",
    "# to converge. When shape or scale is 0, adstock will be 0.\n",
    "\n",
    "## Hill function for saturation: Hill function is a two-parametric function in Robyn with\n",
    "# alpha and gamma. Alpha controls the shape of the curve between exponential and s-shape.\n",
    "# Recommended bound is c(0.5, 3). The larger the alpha, the more S-shape. The smaller, the\n",
    "# more C-shape. Gamma controls the inflexion point. Recommended bounce is c(0.3, 1). The\n",
    "# larger the gamma, the later the inflection point in the response curve.\n",
    "\n",
    "## Regularization for ridge regression: Lambda is the penalty term for regularised regression.\n",
    "# Lambda doesn't need manual definition from the users, because it is set to the range of\n",
    "# c(0, 1) by default in hyperparameters and will be scaled to the proper altitude with\n",
    "# lambda_max and lambda_min_ratio.\n",
    "\n",
    "## Time series validation: When ts_validation = TRUE in robyn_run(), train_size defines the\n",
    "# percentage of data used for training, validation and out-of-sample testing. For example,\n",
    "# when train_size = 0.7, val_size and test_size will be 0.15 each. This hyperparameter is\n",
    "# customizable with default range of c(0.5, 0.8) and must be between c(0.1, 1).\n",
    "\n",
    "## 4. Set individual hyperparameter bounds. They either contain two values e.g. c(0, 0.5),\n",
    "# or only one value, in which case you'd \"fix\" that hyperparameter.\n",
    "# Run hyper_limits() to check maximum upper and lower bounds by range\n",
    "hyper_limits()\n",
    "\n",
    "# Example hyperparameters ranges for Geometric adstock\n",
    "hyperparameters <- list(\n",
    "  facebook_S_alphas = c(0.5, 3),\n",
    "  facebook_S_gammas = c(0.3, 1),\n",
    "  facebook_S_thetas = c(0, 0.3),\n",
    "  print_S_alphas = c(0.5, 3),\n",
    "  print_S_gammas = c(0.3, 1),\n",
    "  print_S_thetas = c(0.1, 0.4),\n",
    "  tv_S_alphas = c(0.5, 3),\n",
    "  tv_S_gammas = c(0.3, 1),\n",
    "  tv_S_thetas = c(0.3, 0.8),\n",
    "  search_S_alphas = c(0.5, 3),\n",
    "  search_S_gammas = c(0.3, 1),\n",
    "  search_S_thetas = c(0, 0.3),\n",
    "  ooh_S_alphas = c(0.5, 3),\n",
    "  ooh_S_gammas = c(0.3, 1),\n",
    "  ooh_S_thetas = c(0.1, 0.4),\n",
    "  newsletter_alphas = c(0.5, 3),\n",
    "  newsletter_gammas = c(0.3, 1),\n",
    "  newsletter_thetas = c(0.1, 0.4),\n",
    "  train_size = c(0.5, 0.8)\n",
    ")\n",
    "\n",
    "# Example hyperparameters ranges for Weibull CDF adstock\n",
    "# facebook_S_alphas = c(0.5, 3)\n",
    "# facebook_S_gammas = c(0.3, 1)\n",
    "# facebook_S_shapes = c(0, 2)\n",
    "# facebook_S_scales = c(0, 0.1)\n",
    "\n",
    "# Example hyperparameters ranges for Weibull PDF adstock\n",
    "# facebook_S_alphas = c(0.5, 3)\n",
    "# facebook_S_gammas = c(0.3, 1)\n",
    "# facebook_S_shapes = c(0, 10)\n",
    "# facebook_S_scales = c(0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79b9f0-5196-4f01-883b-b2ae35faa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2a-3: Third, add hyperparameters into robyn_inputs()\n",
    "\n",
    "InputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)\n",
    "print(InputCollect)\n",
    "\n",
    "#### 2a-4: Fourth (optional), model calibration / add experimental input\n",
    "\n",
    "## Guide for calibration\n",
    "\n",
    "# 1. Calibration channels need to be paid_media_spends or organic_vars names.\n",
    "# 2. We strongly recommend to use Weibull PDF adstock for more degree of freedom when\n",
    "# calibrating Robyn.\n",
    "# 3. We strongly recommend to use experimental and causal results that are considered\n",
    "# ground truth to calibrate MMM. Usual experiment types are identity-based (e.g. Facebook\n",
    "# conversion lift) or geo-based (e.g. Facebook GeoLift). Due to the nature of treatment\n",
    "# and control groups in an experiment, the result is considered immediate effect. It's\n",
    "# rather impossible to hold off historical carryover effect in an experiment. Therefore,\n",
    "# only calibrates the immediate and the future carryover effect. When calibrating with\n",
    "# causal experiments, use calibration_scope = \"immediate\".\n",
    "# 4. It's controversial to use attribution/MTA contribution to calibrate MMM. Attribution\n",
    "# is considered biased towards lower-funnel channels and strongly impacted by signal\n",
    "# quality. When calibrating with MTA, use calibration_scope = \"immediate\".\n",
    "# 5. Every MMM is different. It's highly contextual if two MMMs are comparable or not.\n",
    "# In case of using other MMM result to calibrate Robyn, use calibration_scope = \"total\".\n",
    "# 6. Currently, Robyn only accepts point-estimate as calibration input. For example, if\n",
    "# 10k$ spend is tested against a hold-out for channel A, then input the incremental\n",
    "# return as point-estimate as the example below.\n",
    "# 7. The point-estimate has to always match the spend in the variable. For example, if\n",
    "# channel A usually has $100K weekly spend and the experimental holdout is 70%, input\n",
    "# the point-estimate for the $30K, not the $70K.\n",
    "# 8. If an experiment contains more than one media variable, input \"channe_A+channel_B\"\n",
    "# to indicate combination of channels, case sensitive.\n",
    "\n",
    "# calibration_input <- data.frame(\n",
    "#   # channel name must in paid_media_vars\n",
    "#   channel = c(\"facebook_S\",  \"tv_S\", \"facebook_S+search_S\", \"newsletter\"),\n",
    "#   # liftStartDate must be within input data range\n",
    "#   liftStartDate = as.Date(c(\"2018-05-01\", \"2018-04-03\", \"2018-07-01\", \"2017-12-01\")),\n",
    "#   # liftEndDate must be within input data range\n",
    "#   liftEndDate = as.Date(c(\"2018-06-10\", \"2018-06-03\", \"2018-07-20\", \"2017-12-31\")),\n",
    "#   # Provided value must be tested on same campaign level in model and same metric as dep_var_type\n",
    "#   liftAbs = c(400000, 300000, 700000, 200),\n",
    "#   # Spend within experiment: should match within a 10% error your spend on date range for each channel from dt_input\n",
    "#   spend = c(421000, 7100, 350000, 0),\n",
    "#   # Confidence: if frequentist experiment, you may use 1 - pvalue\n",
    "#   confidence = c(0.85, 0.8, 0.99, 0.95),\n",
    "#   # KPI measured: must match your dep_var\n",
    "#   metric = c(\"revenue\", \"revenue\", \"revenue\", \"revenue\"),\n",
    "#   # Either \"immediate\" or \"total\". For experimental inputs like Facebook Lift, \"immediate\" is recommended.\n",
    "#   calibration_scope = c(\"immediate\", \"immediate\", \"immediate\", \"immediate\")\n",
    "# )\n",
    "# InputCollect <- robyn_inputs(InputCollect = InputCollect, calibration_input = calibration_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bf87f-cf52-410b-b9df-16d8bea07ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Step 2b: For known model specification, setup in one single step\n",
    "\n",
    "## Specify hyperparameters as in 2a-2 and optionally calibration as in 2a-4 and provide them directly in robyn_inputs()\n",
    "\n",
    "# InputCollect <- robyn_inputs(\n",
    "#   dt_input = dt_simulated_weekly\n",
    "#   ,dt_holidays = dt_prophet_holidays\n",
    "#   ,date_var = \"DATE\"\n",
    "#   ,dep_var = \"revenue\"\n",
    "#   ,dep_var_type = \"revenue\"\n",
    "#   ,prophet_vars = c(\"trend\", \"season\", \"holiday\")\n",
    "#   ,prophet_country = \"DE\"\n",
    "#   ,context_vars = c(\"competitor_sales_B\", \"events\")\n",
    "#   ,paid_media_spends = c(\"tv_S\", \"ooh_S\",\t\"print_S\", \"facebook_S\", \"search_S\")\n",
    "#   ,paid_media_vars = c(\"tv_S\", \"ooh_S\", \t\"print_S\", \"facebook_I\", \"search_clicks_P\")\n",
    "#   ,organic_vars = c(\"newsletter\")\n",
    "#   ,factor_vars = c(\"events\")\n",
    "#   ,window_start = \"2016-11-23\"\n",
    "#   ,window_end = \"2018-08-22\"\n",
    "#   ,adstock = \"geometric\"\n",
    "#   ,hyperparameters = hyperparameters # as in 2a-2 above\n",
    "#   ,calibration_input = calibration_input # as in 2a-4 above\n",
    "# )\n",
    "\n",
    "#### Check spend exposure fit if available\n",
    "if (length(InputCollect$exposure_vars) > 0) {\n",
    "  lapply(InputCollect$modNLS$plots, plot)\n",
    "}\n",
    "\n",
    "##### Manually save and import InputCollect as JSON file\n",
    "# robyn_write(InputCollect, dir = \"~/Desktop\")\n",
    "# InputCollect <- robyn_inputs(\n",
    "#   dt_input = dt_simulated_weekly,\n",
    "#   dt_holidays = dt_prophet_holidays,\n",
    "#   json_file = \"~/Desktop/RobynModel-inputs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08dc60-f813-414a-a452-abec7770327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Step 3: Build initial model\n",
    "\n",
    "## Run all trials and iterations. Use ?robyn_run to check parameter definition\n",
    "OutputModels <- robyn_run(\n",
    "  InputCollect = InputCollect, # feed in all model specification\n",
    "  cores = NULL, # NULL defaults to (max available - 1)\n",
    "  iterations = 2000, # 2000 recommended for the dummy dataset with no calibration\n",
    "  trials = 5, # 5 recommended for the dummy dataset\n",
    "  ts_validation = TRUE, # 3-way-split time series for NRMSE validation.\n",
    "  add_penalty_factor = FALSE # Experimental feature. Use with caution.\n",
    ")\n",
    "print(OutputModels)\n",
    "\n",
    "## Check MOO (multi-objective optimization) convergence plots\n",
    "# Read more about convergence rules: ?robyn_converge\n",
    "OutputModels$convergence$moo_distrb_plot\n",
    "OutputModels$convergence$moo_cloud_plot\n",
    "\n",
    "## Check time-series validation plot (when ts_validation == TRUE)\n",
    "# Read more and replicate results: ?ts_validation\n",
    "if (OutputModels$ts_validation) OutputModels$ts_validation_plot\n",
    "\n",
    "## Calculate Pareto fronts, cluster and export results and plots. See ?robyn_outputs\n",
    "OutputCollect <- robyn_outputs(\n",
    "  InputCollect, OutputModels,\n",
    "  pareto_fronts = \"auto\", # automatically pick how many pareto-fronts to fill min_candidates (100)\n",
    "  # min_candidates = 100, # top pareto models for clustering. Default to 100\n",
    "  # calibration_constraint = 0.1, # range c(0.01, 0.1) & default at 0.1\n",
    "  csv_out = \"pareto\", # \"pareto\", \"all\", or NULL (for none)\n",
    "  clusters = TRUE, # Set to TRUE to cluster similar models by ROAS. See ?robyn_clusters\n",
    "  export = create_files, # this will create files locally\n",
    "  plot_folder = robyn_directory, # path for plots exports and files creation\n",
    "  plot_pareto = create_files # Set to FALSE to deactivate plotting and saving model one-pagers\n",
    ")\n",
    "print(OutputCollect)\n",
    "\n",
    "## 4 csv files are exported into the folder for further usage. Check schema here:\n",
    "## https://github.com/facebookexperimental/Robyn/blob/main/demo/schema.R\n",
    "# pareto_hyperparameters.csv, hyperparameters per Pareto output model\n",
    "# pareto_aggregated.csv, aggregated decomposition per independent variable of all Pareto output\n",
    "# pareto_media_transform_matrix.csv, all media transformation vectors\n",
    "# pareto_alldecomp_matrix.csv, all decomposition vectors of independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d99c1c-496d-4b05-bc01-4c29c071b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Step 4: Select and save the any model\n",
    "\n",
    "## Compare all model one-pagers and select one that mostly reflects your business reality\n",
    "print(OutputCollect)\n",
    "select_model <- \"1_122_7\" # Pick one of the models from OutputCollect to proceed\n",
    "\n",
    "#### Version >=3.7.1: JSON export and import (faster and lighter than RDS files)\n",
    "ExportedModel <- robyn_write(InputCollect, OutputCollect, select_model, export = create_files)\n",
    "print(ExportedModel)\n",
    "\n",
    "# To plot any model's one-pager:\n",
    "myOnePager <- robyn_onepagers(InputCollect, OutputCollect, select_model, export = FALSE)\n",
    "\n",
    "# To check each of the one-pager's plots\n",
    "# myOnePager[[select_model]]$patches$plots[[1]]\n",
    "# myOnePager[[select_model]]$patches$plots[[2]]\n",
    "# myOnePager[[select_model]]$patches$plots[[3]] # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59848ab3-f117-4231-9988-0cf44c2eb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Step 5: Get budget allocation based on the selected model above\n",
    "\n",
    "## Budget allocation result requires further validation. Please use this recommendation with caution.\n",
    "## Don't interpret budget allocation result if selected model above doesn't meet business expectation.\n",
    "\n",
    "# Check media summary for selected model\n",
    "print(ExportedModel)\n",
    "\n",
    "# Run ?robyn_allocator to check parameter definition\n",
    "\n",
    "# NOTE: The order of constraints should follow:\n",
    "InputCollect$paid_media_spends\n",
    "\n",
    "# Scenario \"max_response\": \"What's the max. return given certain spend?\"\n",
    "# Example 1: max_response default setting: maximize response for latest month\n",
    "AllocatorCollect1 <- robyn_allocator(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  # date_range = \"all\", # Default to \"all\"\n",
    "  # total_budget = NULL, # When NULL, default is total spend in date_range\n",
    "  channel_constr_low = 0.7,\n",
    "  channel_constr_up = c(1.2, 1.5, 1.5, 1.5, 1.5),\n",
    "  # channel_constr_multiplier = 3,\n",
    "  scenario = \"max_response\",\n",
    "  export = create_files\n",
    ")\n",
    "# Print & plot allocator's output\n",
    "print(AllocatorCollect1)\n",
    "plot(AllocatorCollect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd1ee8-3994-4b1f-a963-f8839a545e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: maximize response for latest 10 periods with given spend\n",
    "AllocatorCollect2 <- robyn_allocator(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  date_range = \"last_10\", # Last 10 periods, same as c(\"2018-10-22\", \"2018-12-31\")\n",
    "  total_budget = 5000000, # Total budget for date_range period simulation\n",
    "  channel_constr_low = c(0.8, 0.7, 0.7, 0.7, 0.7),\n",
    "  channel_constr_up = c(1.2, 1.5, 1.5, 1.5, 1.5),\n",
    "  channel_constr_multiplier = 5, # Customise bound extension for wider insights\n",
    "  scenario = \"max_response\",\n",
    "  export = create_files\n",
    ")\n",
    "print(AllocatorCollect2)\n",
    "plot(AllocatorCollect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775a1e0-cd1b-4c4c-b23f-d25aae3b4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario \"target_efficiency\": \"How much to spend to hit ROAS or CPA of x?\"\n",
    "# Example 3: Use default ROAS target for revenue or CPA target for conversion\n",
    "# Check InputCollect$dep_var_type for revenue or conversion type\n",
    "# Two default ROAS targets: 0.8x of initial ROAS as well as ROAS = 1\n",
    "# Two default CPA targets: 1.2x and 2.4x of the initial CPA\n",
    "AllocatorCollect3 <- robyn_allocator(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  # date_range = NULL, # Default last month as initial period\n",
    "  scenario = \"target_efficiency\",\n",
    "  # target_value = 2, # Customize target ROAS or CPA value\n",
    "  export = create_files\n",
    ")\n",
    "print(AllocatorCollect3)\n",
    "plot(AllocatorCollect3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99fd57-4e6d-44aa-80e5-8b6a12e6c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Customize target_value for ROAS or CPA (using json_file)\n",
    "json_file = \"~/Desktop/Robyn_202302221206_init/RobynModel-1_117_11.json\"\n",
    "AllocatorCollect4 <- robyn_allocator(\n",
    "  json_file = json_file, # Using json file from robyn_write() for allocation\n",
    "  dt_input = dt_simulated_weekly,\n",
    "  dt_holidays = dt_prophet_holidays,\n",
    "  date_range = NULL, # Default last month as initial period\n",
    "  scenario = \"target_efficiency\",\n",
    "  target_value = 2, # Customize target ROAS or CPA value\n",
    "  plot_folder = \"~/Desktop/my_dir\",\n",
    "  plot_folder_sub = \"my_subdir\",\n",
    "  export = create_files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1609f45-8c3b-4a4d-a5e5-1d45f6785c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A csv is exported into the folder for further usage. Check schema here:\n",
    "## https://github.com/facebookexperimental/Robyn/blob/main/demo/schema.R\n",
    "\n",
    "## QA optimal response\n",
    "# Pick any media variable: InputCollect$all_media\n",
    "select_media <- \"search_S\"\n",
    "# For paid_media_spends set metric_value as your optimal spend\n",
    "metric_value <- AllocatorCollect1$dt_optimOut$optmSpendUnit[\n",
    "  AllocatorCollect1$dt_optimOut$channels == select_media\n",
    "]; metric_value\n",
    "# # For paid_media_vars and organic_vars, manually pick a value\n",
    "# metric_value <- 10000\n",
    "\n",
    "## Saturation curve for adstocked metric results (example)\n",
    "robyn_response(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  metric_name = select_media,\n",
    "  metric_value = metric_value,\n",
    "  date_range = \"last_5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6397ec-5204-48fc-8e23-861d0c9f68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Step 6: Model refresh based on selected model and saved results\n",
    "\n",
    "## Must run robyn_write() (manually or automatically) to export any model first, before refreshing.\n",
    "## The robyn_refresh() function is suitable for updating within \"reasonable periods\".\n",
    "## Two situations are considered better to rebuild model:\n",
    "## 1. most data is new. If initial model has 100 weeks and 80 weeks new data is added in refresh,\n",
    "## it might be better to rebuild the model. Rule of thumb: 50% of data or less can be new.\n",
    "## 2. new variables are added.\n",
    "\n",
    "# Provide JSON file with your InputCollect and ExportedModel specifications\n",
    "# It can be any model, initial or a refresh model\n",
    "json_file <- \"~/Desktop/Robyn_202211211853_init/RobynModel-1_100_6.json\"\n",
    "RobynRefresh <- robyn_refresh(\n",
    "  json_file = json_file,\n",
    "  dt_input = dt_simulated_weekly,\n",
    "  dt_holidays = dt_prophet_holidays,\n",
    "  refresh_steps = 13,\n",
    "  refresh_iters = 1000, # 1k is an estimation\n",
    "  refresh_trials = 1\n",
    ")\n",
    "# Now refreshing a refreshed model, following the same approach\n",
    "json_file_rf1 <- \"~/Desktop/Robyn_202208231837_init/Robyn_202208231841_rf1/RobynModel-1_12_5.json\"\n",
    "RobynRefresh <- robyn_refresh(\n",
    "  json_file = json_file_rf1,\n",
    "  dt_input = dt_simulated_weekly,\n",
    "  dt_holidays = dt_prophet_holidays,\n",
    "  refresh_steps = 7,\n",
    "  refresh_iters = 1000, # 1k is an estimation\n",
    "  refresh_trials = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9764d04-5ad0-4aaf-a7e7-a805d0a857df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with refreshed new InputCollect, OutputCollect, select_model values\n",
    "InputCollectX <- RobynRefresh$listRefresh1$InputCollect\n",
    "OutputCollectX <- RobynRefresh$listRefresh1$OutputCollect\n",
    "select_modelX <- RobynRefresh$listRefresh1$OutputCollect$selectID\n",
    "\n",
    "## Besides plots: there are 4 CSV outputs saved in the folder for further usage\n",
    "# report_hyperparameters.csv, hyperparameters of all selected model for reporting\n",
    "# report_aggregated.csv, aggregated decomposition per independent variable\n",
    "# report_media_transform_matrix.csv, all media transformation vectors\n",
    "# report_alldecomp_matrix.csv,all decomposition vectors of independent variables\n",
    "\n",
    "\n",
    "################################################################\n",
    "#### Step 7: get marginal returns\n",
    "\n",
    "## Example of how to get marginal ROI of next 1000$ from the 80k spend level for search channel\n",
    "\n",
    "# Run ?robyn_response to check parameter definition\n",
    "\n",
    "## The robyn_response() function can now output response for both spends and exposures (imps,\n",
    "## GRP, newsletter sendings etc.) as well as plotting individual saturation curves. New\n",
    "## argument names \"metric_name\" and \"metric_value\" instead of \"paid_media_var\" and \"spend\"\n",
    "## are now used to accommodate this change. Also the returned output is a list now and\n",
    "## contains also the plot.\n",
    "\n",
    "## Recreate original saturation curve\n",
    "Response <- robyn_response(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  metric_name = \"facebook_S\"\n",
    ")\n",
    "Response$plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4529728-eebd-4972-82eb-cae0ec65a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Or you can call a JSON file directly (a bit slower)\n",
    "# Response <- robyn_response(\n",
    "#   json_file = \"your_json_path.json\",\n",
    "#   dt_input = dt_simulated_weekly,\n",
    "#   dt_holidays = dt_prophet_holidays,\n",
    "#   metric_name = \"facebook_S\"\n",
    "# )\n",
    "\n",
    "## Get the \"next 100 dollar\" marginal response on Spend1\n",
    "Spend1 <- 20000\n",
    "Response1 <- robyn_response(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  metric_name = \"facebook_S\",\n",
    "  metric_value = Spend1, # total budget for date_range\n",
    "  date_range = \"last_1\" # last two periods\n",
    ")\n",
    "Response1$plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e2a3c-cb52-42a2-af3f-f321573c30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spend2 <- Spend1 + 100\n",
    "Response2 <- robyn_response(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  metric_name = \"facebook_S\",\n",
    "  metric_value = Spend2,\n",
    "  date_range = \"last_1\"\n",
    ")\n",
    "# ROAS for the 100$ from Spend1 level\n",
    "(Response2$response_total - Response1$response_total) / (Spend2 - Spend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd25766-565e-41c5-a960-050dfbfcfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get response from for a given budget and date_range\n",
    "Spend3 <- 100000\n",
    "Response3 <- robyn_response(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  metric_name = \"facebook_S\",\n",
    "  metric_value = Spend3, # total budget for date_range\n",
    "  date_range = \"last_5\" # last 5 periods\n",
    ")\n",
    "Response3$plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b5f5e-6f29-4b62-bda7-4e0ab06f28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of getting paid media exposure response curves\n",
    "# imps <- 10000000\n",
    "# response_imps <- robyn_response(\n",
    "#   InputCollect = InputCollect,\n",
    "#   OutputCollect = OutputCollect,\n",
    "#   select_model = select_model,\n",
    "#   metric_name = \"facebook_I\",\n",
    "#   metric_value = imps\n",
    "# )\n",
    "# response_imps$response_total / imps * 1000\n",
    "# response_imps$plot\n",
    "\n",
    "## Example of getting organic media exposure response curves\n",
    "sendings <- 30000\n",
    "response_sending <- robyn_response(\n",
    "  InputCollect = InputCollect,\n",
    "  OutputCollect = OutputCollect,\n",
    "  select_model = select_model,\n",
    "  metric_name = \"newsletter\",\n",
    "  metric_value = sendings\n",
    ")\n",
    "# response per 1000 sendings\n",
    "response_sending$response_total / sendings * 1000\n",
    "response_sending$plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13175857-c77a-41ee-807a-cda2cc3e56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### Optional: recreate old models and replicate results\n",
    "\n",
    "# From an exported JSON file (which is created automatically when exporting a model)\n",
    "# we can re-create a previously trained model and outputs. Note: we need to provide\n",
    "# the main dataset and the holidays dataset, which are NOT stored in the JSON file.\n",
    "# These JSON files will be automatically created in most cases.\n",
    "\n",
    "############ WRITE ############\n",
    "# Manually create JSON file with inputs data only\n",
    "robyn_write(InputCollect, dir = \"~/Desktop\")\n",
    "\n",
    "# Manually create JSON file with inputs and specific model results\n",
    "robyn_write(InputCollect, OutputCollect, select_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4641839-467b-425a-a07f-14453655d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ READ ############\n",
    "# Recreate `InputCollect` and `OutputCollect` objects\n",
    "# Pick any exported model (initial or refreshed)\n",
    "json_file <- \"~/Desktop/Robyn_202208231837_init/RobynModel-1_100_6.json\"\n",
    "\n",
    "# Optional: Manually read and check data stored in file\n",
    "json_data <- robyn_read(json_file)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0cf8d-b91d-4386-b453-d5af7f949957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create InputCollect\n",
    "InputCollectX <- robyn_inputs(\n",
    "  dt_input = dt_simulated_weekly,\n",
    "  dt_holidays = dt_prophet_holidays,\n",
    "  json_file = json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec3511-6aa3-4b52-87a8-8ca8466fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create OutputCollect\n",
    "OutputCollectX <- robyn_run(\n",
    "  InputCollect = InputCollectX,\n",
    "  json_file = json_file,\n",
    "  export = create_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef107b6c-6945-4959-bc0a-c9b8a7d0e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or re-create both by simply using robyn_recreate()\n",
    "RobynRecreated <- robyn_recreate(\n",
    "  json_file = \"~/Desktop/Robyn_202303131448_init/RobynModel-1_103_7.json\",\n",
    "  dt_input = dt_simulated_weekly,\n",
    "  dt_holidays = dt_prophet_holidays,\n",
    "  quiet = FALSE)\n",
    "InputCollectX <- RobynRecreated$InputCollect\n",
    "OutputCollectX <- RobynRecreated$OutputCollect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc0dba-ec49-466f-a34e-529eec0f9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-export or rebuild a model and check summary\n",
    "myModel <- robyn_write(InputCollectX, OutputCollectX, export = FALSE, dir = \"~/Desktop\")\n",
    "print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe507c21-6a7d-4952-91f2-aa87623f693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create one-pager\n",
    "myModelPlot <- robyn_onepagers(InputCollectX, OutputCollectX, export = FALSE)\n",
    "# myModelPlot[[1]]$patches$plots[[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e9cb7-cfda-4f9e-bb80-11512b1df568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh any imported model\n",
    "RobynRefresh <- robyn_refresh(\n",
    "  json_file = json_file,\n",
    "  dt_input = InputCollectX$dt_input,\n",
    "  dt_holidays = InputCollectX$dt_holidays,\n",
    "  refresh_steps = 6,\n",
    "  refresh_mode = \"manual\",\n",
    "  refresh_iters = 1000,\n",
    "  refresh_trials = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098fa2f-d5cf-48e7-8025-e587a44d3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate response curves\n",
    "robyn_response(\n",
    "  InputCollect = InputCollectX,\n",
    "  OutputCollect = OutputCollectX,\n",
    "  metric_name = \"newsletter\",\n",
    "  metric_value = 50000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
