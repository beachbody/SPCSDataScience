{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Walk Through Jupyter Lab SPCS\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: False\n",
    "        output: False\n",
    "    revealjs:\n",
    "        embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can go directly to this [github](https://github.com/sfc-gh-jdemlow/SPCSDataScience) and run this locally and follow along for the best results. If you run into errors please leave comments and I will do my best to get to them and make changes as necessary. \n",
    "\n",
    "# Youtube Links:\n",
    "\n",
    "Still need to record..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to my first Snowflake article please leave comments on how I can improve my articles or even what I did well so that I can make this better in future parts. This will be a series that I and my colleagues will continue to develop as new and improved features come out that enhance the Snowflake platform.\n",
    "\n",
    "You can follow the most up to date version of this development series at this github repository. \n",
    "The branch name for this part will be origin as the main branch will be the latest version of these parts.\n",
    "\n",
    "> The branch name for this article will be `origin`\n",
    "\n",
    "\n",
    "## What?\n",
    "\n",
    "We are addressing with SPCS is a development space for data scientist that Snowflake currently does not have out of the box. Even though snowflake notebooks are coming seen here. There is some time until they are generally available. Additionally, this method will give you the ability to use git read/write. Snowflake recently introduced a read only Git integrations, but the write capabilities is actively being worked on. In the meantime, SPCS provides a good solution. SPCS also allows you to bring your own libraries or libraries that might not be supported in the Snowflake ecosystem. So by creating an image that you need and installing the Git integration of your choice you will be able to interact with your git repository inside of snowflake. This is a game changer for data scientist and machine learning engineers being able to securely work in the snowflake ecosystem.\n",
    "\n",
    "\n",
    "### In This Example:\n",
    "\n",
    "This GitHub repository it demonstrates that you can have a basic but complete CI/CD process centered around Snowflake. The Github Action in this repo conducts both training and inference of a model in the same sequence, but the idea is that you can schedule your inference or training with any tool. You can also use snowflake labs action here, but I had already written the one you seen in this repo before it's release so feel free to adjust as you see fit\n",
    "\n",
    "In future articles we will be working through Snowflake's ML capabilities and adding them to this series. Some of those features will be native snowflake preprocessing, a feature store, machine learning modeling, model registries, model management, cortex baseline models and more so make sure to come back to see how this series grows.\n",
    "\n",
    "If you don't want to use github actions and want to use a snowflake task to execute the docker image you can do it in the following way\n",
    "\n",
    "\n",
    "::: {.callout-note title=\"Task Example of Executing Docker Image\" collapse=\"true\"}\n",
    "\n",
    "Here is an example of what this would look like as a task.\n",
    "\n",
    "```sql \n",
    "\n",
    "create or replace task DEMO_TASK_1\n",
    "USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n",
    "SCHEDULE ='180 minutes'\n",
    "as\n",
    "EXECUTE SERVICE\n",
    "    IN COMPUTE POOL CONTAINER_DEMO_POOL\n",
    "    FROM SPECIFICATION $$\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: modeling\n",
    "          image: sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo/modeling\n",
    "          env:\n",
    "            SNOWFLAKE_WAREHOUSE: CONTAINER_DEMO_WH\n",
    "            NOTEBOOK_FILE: \"nbs/99a_train_randomforest.ipynb\"\n",
    "          command:\n",
    "            - \"/bin/bash\"\n",
    "            - \"-c\"\n",
    "            - \"source /opt/venv/bin/activate && { datetime=$(date +'%Y%m%d_%H%M%S'); jupyter nbconvert --execute --inplace --allow-errors --ExecutePreprocessor.timeout=-1 --NotebookApp.token='${JUPYTER_TOKEN}' ${NOTEBOOK_FILE} --output=executed_${datetime}.ipynb; }\"\n",
    "          volumeMounts:\n",
    "            - name: juypter-nbs\n",
    "              mountPath: /home/jupyter\n",
    "      networkPolicyConfig:\n",
    "          allowInternetEgress: true\n",
    "      volumes:\n",
    "        - name: juypter-nbs\n",
    "          source: \"@volumes/juypter-nbs\"\n",
    "          uid: 1000\n",
    "          gid: 1000\n",
    "    \n",
    "    $$\n",
    "\n",
    "```\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "## How?\n",
    "\n",
    "Through the power of Snowpark Container Services (SPCS), we are now able to work on the platform of our choice Snowflake while having the flexibility to work in the way that we prefer. Throughout this series, you will discover that there are truly endless possibilities with Snowflake.\n",
    "\n",
    "Our Docker image will include persistent volumes that provide read and write access to internal stages within Snowflake, allowing you to log into your secure Snowflake ecosystem and pull your repo in and develop while using the variety of compute layers offered by snowflake. As your development may need large compute pools snowflake will be able to do that an even bring your development experience GPUs seamlessly and with in a second due to it's elastic compute layer.\n",
    "Note: There are numerous talented individuals and companies already doing amazing work with it. It is important to clarify that this is not the only approach, but it should help you get started.\n",
    "\n",
    "> Note: There are numerous talented individuals and companies already doing amazing work with it. It is important to clarify that this is not the only approach, but it should help you get started.\n",
    "\n",
    "\n",
    "## Why? \n",
    "\n",
    "With over 8 years of experience as a Data Scientist and Machine Learning Engineer, I have always desired the capability to handle all our needs directly within Snowflake. Now, thanks to Snowpark Container Services (SPCS), this is possible without needing to switch platforms. My approach has been greatly influenced by the notebook development methodology popularized by Fast.ai through nbdev, and you will see a Jupyter Notebook-first approach to development in Snowflake throughout this series.\n",
    "\n",
    "Jeremy Howard has been an incredible mentor to numerous outstanding open-source developers, and I would like to take this opportunity to thank him for his contributions to the community. While you may not agree with this development strategy, it's important to remember that it's flexible. If you prefer a different IDE, you can modify the Docker image to support Visual Studio Code and continue your work there. We plan to use Jupyter Lab to ensure accessibility for everyone, and future versions will likely include support for the Visual Studio Code setup. Initially, our Jupyter Lab setup will only support Python, but it is designed to be adaptable to other programming environments like R Studio, Scala, or any other language that fits your development preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walk Through\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure that you have the following:\n",
    "\n",
    "- Docker installed and configured on your local machine\n",
    "\n",
    "- A GitHub account with a repository set up for your project\n",
    "\n",
    "- Access to a Snowflake instance with the necessary permissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Local Conda Environment\n",
    "\n",
    "> **Warning**: You must have Conda installed on your computer already before you can proceed with this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output: false\n",
    "! conda create --name snowpark --override-channels -c https://repo.anaconda.com/pkgs/snowflake python=3.10 --y; conda activate snowpark; pip install -r ../infra/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, you will want to switch your kernel to `snowpark`. You might need to close Jupyter and reopen it to switch to the correct environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Symlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Symlink for Notebook\n",
    "! ln -s .../DataScience/ ./DataScience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataScience.connection import *\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Session and Execute Set Up\n",
    "\n",
    "> If you don't have your enviornment variables set for this then you will want to change the next cell to the following cell. There is also the assumption that you have a database, schema, warehouse and role for this article to work. You will also at some point need to ask your account admin to give you the correct permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : JD_SERVICE_ACCOUNT_ADMIN\n",
      "Role                        : \"ML_USER_ROLE\"\n",
      "Database                    : \"CLASSIFICATION\"\n",
      "Schema                      : \"PROD\"\n",
      "Warehouse                   : \"ML_WH\"\n",
      "Snowflake version           : 8.16.0\n",
      "Snowpark for Python version : 1.14.0\n"
     ]
    }
   ],
   "source": [
    "session = create_snowflake_session(\n",
    "    database='Classification',\n",
    "    schema='PROD',\n",
    "    warehouse='ML_WH',\n",
    "    role='ML_USER_ROLE',\n",
    ")\n",
    "session.sql_simplifier_enabled = True\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output: false\n",
    "execute_sql_file(\n",
    "    session,\n",
    "    file_path='DataScience/files/sql/00_setup.sql'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SetUp Creates](./images/00_setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make We want to ensure our image repository was created, so we will run the following command:\n",
    " out image repository was created so we will run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>owner</th>\n",
       "      <th>owner_role_type</th>\n",
       "      <th>comment</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-25 09:22:31.178000-07:00</td>\n",
       "      <td>IMAGE_REPO</td>\n",
       "      <td>CONTAINER_DEMO_DB</td>\n",
       "      <td>PUBLIC</td>\n",
       "      <td>CONTAINER_USER_ROLE</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>This is for modeling useage</td>\n",
       "      <td>sfsenorthamerica-demo-jdemlow.registry.snowfla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on        name      database_name schema_name  \\\n",
       "0 2024-04-25 09:22:31.178000-07:00  IMAGE_REPO  CONTAINER_DEMO_DB      PUBLIC   \n",
       "\n",
       "                 owner owner_role_type                      comment  \\\n",
       "0  CONTAINER_USER_ROLE            ROLE  This is for modeling useage   \n",
       "\n",
       "                                      repository_url  \n",
       "0  sfsenorthamerica-demo-jdemlow.registry.snowfla...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing 'repository_url' column from output to secure the URL link\n",
    "display(pd.DataFrame(session.sql(f\"SHOW IMAGE REPOSITORIES;\").collect())[['created_on', 'name', 'database_name', 'schema_name', 'owner', 'owner_role_type', 'comment', 'repository_url']])\n",
    "pd.DataFrame(session.sql(f\"SHOW IMAGE REPOSITORIES;\").collect())[['repository_url']].values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^string above is going to be your repository that you will put into the cells below to make your make file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note title=\"Explain Setup Execution Code\" collapse=\"true\"}\n",
    "\n",
    "# Snowflake Setup for Container Usage\n",
    "\n",
    "This documentation outlines the steps to configure Snowflake for container usage, emphasizing on roles, warehouses, databases, and stages setup, specifically tailored for modeling and development purposes.\n",
    "\n",
    "## Step 1: Create a Role with Required Privileges\n",
    "\n",
    "First, we initiate the process by switching to the `ACCOUNTADMIN` role to create a new role named `CONTAINER_USER_ROLE`. This role is then granted a set of privileges that are essential for the management and operation of databases, warehouses, and integrations within Snowflake. These privileges are crucial for creating and managing the infrastructure needed for containerized applications.\n",
    "\n",
    "```sql\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE ROLE CONTAINER_USER_ROLE;\n",
    "GRANT CREATE DATABASE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT CREATE COMPUTE POOL ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT MONITOR USAGE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE CONTAINER_USER_ROLE;\n",
    "```\n",
    "\n",
    "## Step 2: Grant the New Role to `ACCOUNTADMIN`\n",
    "\n",
    "After creating the `CONTAINER_USER_ROLE`, it's necessary to grant this role to the `ACCOUNTADMIN` to ensure the administrator can utilize this new role. This step integrates the newly created role within the organizational roles hierarchy, allowing for seamless role management and access control.\n",
    "\n",
    "```sql\n",
    "GRANT ROLE CONTAINER_USER_ROLE TO ROLE ACCOUNTADMIN;\n",
    "```\n",
    "\n",
    "## Step 3: Create and Configure a Warehouse\n",
    "\n",
    "Next, we focus on setting up a warehouse named `CONTAINER_DEMO_WH` with specified parameters to support the operational needs of container usage. This includes defining the warehouse size, auto-suspend, and auto-resume settings to optimize resource utilization.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE WAREHOUSE CONTAINER_DEMO_WH\n",
    "WAREHOUSE_SIZE = XSMALL\n",
    "AUTO_SUSPEND = 120\n",
    "AUTO_RESUME = TRUE;\n",
    "GRANT OWNERSHIP ON WAREHOUSE CONTAINER_DEMO_WH TO ROLE CONTAINER_USER_ROLE REVOKE CURRENT GRANTS;\n",
    "```\n",
    "\n",
    "## Step 4: Setup Database and Stages\n",
    "\n",
    "With the `CONTAINER_USER_ROLE`, we proceed to create a database `CONTAINER_DEMO_DB` and configure stages for different purposes, including specs, volumes, and RDF models. Each stage is set up with encryption and directory enablement, tailored for specific use cases such as storing Docker image commands, data development, and modeling purposes.\n",
    "\n",
    "```sql\n",
    "USE ROLE CONTAINER_USER_ROLE;\n",
    "CREATE OR REPLACE DATABASE CONTAINER_DEMO_DB;\n",
    "USE DATABASE CONTAINER_DEMO_DB;\n",
    "\n",
    "CREATE STAGE IF NOT EXISTS SPECS\n",
    "ENCRYPTION = (TYPE='SNOWFLAKE_SSE')\n",
    "DIRECTORY = (ENABLE = TRUE)\n",
    "COMMENT = 'Store Docker Image Commands for SPECS';\n",
    "\n",
    "CREATE STAGE IF NOT EXISTS VOLUMES\n",
    "ENCRYPTION = (TYPE='SNOWFLAKE_SSE')\n",
    "DIRECTORY = (ENABLE = TRUE)\n",
    "COMMENT = 'Saving Data As We Develop In Snowflake';\n",
    "\n",
    "CREATE STAGE IF NOT EXISTS RDF_MODEL\n",
    "ENCRYPTION = (TYPE='SNOWFLAKE_SSE')\n",
    "DIRECTORY = (ENABLE = TRUE)\n",
    "COMMENT = 'For This Use Case';\n",
    "```\n",
    "\n",
    "## Step 5: Create and Configure Image Repository\n",
    "\n",
    "Finally, an image repository named `IMAGE_REPO` is created within the `CONTAINER_DEMO_DB.PUBLIC` schema for modeling usage. This repository is essential for managing container images, facilitating the development and deployment process. Access rights are granted to the `CONTAINER_USER_ROLE` for both reading and writing, ensuring the role can fully utilize the repository for container management.\n",
    "\n",
    "```sql\n",
    "CREATE IMAGE REPOSITORY CONTAINER_DEMO_DB.PUBLIC.IMAGE_REPO\n",
    "COMMENT = 'This is for modeling usage';\n",
    "\n",
    "GRANT READ ON STAGE CONTAINER_DEMO_DB.PUBLIC.RDF_MODEL TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT WRITE ON STAGE CONTAINER_DEMO_DB.PUBLIC.RDF_MODEL TO ROLE CONTAINER_USER_ROLE;\n",
    "\n",
    "GRANT READ ON IMAGE REPOSITORY CONTAINER_DEMO_DB.PUBLIC.IMAGE_REPO TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT WRITE ON IMAGE REPOSITORY CONTAINER_DEMO_DB.PUBLIC.IMAGE_REPO TO ROLE CONTAINER_USER_ROLE;\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Yaml Files\n",
    "\n",
    "> **Warning**: Set up config.toml with snow sql need to be set up before running configure.sh and ensure Docker is on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository URL: sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\n",
      "Warehouse: CONTAINER_DEMO_WH\n",
      "Image Name: modeling\n",
      "Makefile created from template.\n",
      "modeling.yaml created from template.\n",
      "Placeholder values have been replaced in Makefile and modeling.yaml.\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "# The last output of this will be the url you put here\n",
    "! rm ../modeling.yaml\n",
    "! rm ../Makefile\n",
    "! bash ../infra/configure.sh -r \"sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\" -w \"CONTAINER_DEMO_WH\" -i \"modeling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec:\n",
      "  containers:\n",
      "    - name: modeling\n",
      "      image: sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo/modeling\n",
      "      env:\n",
      "        SNOWFLAKE_WAREHOUSE: CONTAINER_DEMO_WH\n",
      "      volumeMounts:\n",
      "        - name: juypter-nbs\n",
      "          mountPath: /home/jupyter\n",
      "  endpoints:\n",
      "    - name: modeling\n",
      "      port: 8080\n",
      "      public: true\n",
      "  networkPolicyConfig:\n",
      "      allowInternetEgress: true\n",
      "  volumes:\n",
      "    - name: juypter-nbs\n",
      "      source: \"@volumes/juypter-nbs\"\n",
      "      uid: 1000\n",
      "      gid: 1000"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "! cat ../modeling.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move Newly Created Spec File (modeling.yaml) to Stage @specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: modeling.yaml, Status: UPLOADED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>md5</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>specs/modeling.yaml</td>\n",
       "      <td>530</td>\n",
       "      <td>a3d1e68a091ea60647950495bd022360</td>\n",
       "      <td>Thu, 25 Apr 2024 16:22:36 GMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  size                               md5  \\\n",
       "0  specs/modeling.yaml   530  a3d1e68a091ea60647950495bd022360   \n",
       "\n",
       "                   last_modified  \n",
       "0  Thu, 25 Apr 2024 16:22:36 GMT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stage_location = r'@specs'\n",
    "stage_location = stage_location.strip('@')\n",
    "file_path = '../modeling.yaml'\n",
    "\n",
    "put_results = session.file.put(local_file_name=file_path, stage_location=stage_location, auto_compress=False, overwrite=True)\n",
    "for result in put_results:\n",
    "    print(f\"File: {result.source}, Status: {result.status}\")\n",
    "display(pd.DataFrame(session.sql('ls @specs').collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Creation\n",
    "\n",
    "### Run Docker Build Locally\n",
    "\n",
    "> We want you to test this locally by designed to allow you to have a better understanding  ``make build_local; make run`` and make sure it's working as you would expect it then move on to the next steps. \n",
    "\n",
    "1. Download docker desktop and ensure it is open and running.\n",
    "\n",
    "    - > **Warning**: Make sure you have docker running and make sure you have logged in already ``~/.docker/config.json`` you can check this or run docker login in the terminal\n",
    "\n",
    "    - > **Also make sure you make your .env file in infra** for your enviornment vairiables to be used in your docker-compose.yaml. As you grow out of this tutorial mode the next step here would be to create github actions to create these approaches so that you are able to run this in a more devops style approach. In future versions this will be shown.\n",
    "\n",
    "2. Open Terminal navigate to this repo and run ``make build_local``\n",
    "\n",
    "    - If you are using VS code you can simple click terminal and paste the command\n",
    "\n",
    "3. After your local build is complete you can then run ``make run``.\n",
    "\n",
    "    - This is going to be running a docker compose process that can be viewed inside of the infra/ folder\n",
    "\n",
    "\n",
    "#### Pictures of Steps\n",
    "\n",
    "![Make build](./images/make_build_local.png)\n",
    "\n",
    "![Jupyterlab Initial](./images/jupyterlab_1.png)\n",
    "\n",
    "![Jupyterlab Logged In](./images/jupyterlab_2.png)\n",
    "\n",
    "![Jupyterlab Execute Imports](./images/jupyterlab_3.png)\n",
    "\n",
    "> **Note**: As SPCS will not allow your volumes to be respected in a two way methodology we have observed that COPY statements in your dockerfile which land in a directory which are then stage-mounted as a volume will NOT initialize the file in-stage. We have on open product gap for this but has thus far been low-priority currently so we are going to bring this repo to our development experience using the github, gitlab, bitbucket etc integrations later on in this process. So if you are wondering why we are using gh cli to bring the repo to the docker image this is one of the reasons.\n",
    "\n",
    "\n",
    "\n",
    "> You also need to make sure you are able to login in with snowql and snow cli to be able to run the following commands\n",
    "\n",
    "### Pushing Image To Snowflake \n",
    "\n",
    "> make all will --> Login, Create, Push To Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo \"Logging in to sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\"\n",
      "Logging in to sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\n",
      "docker login sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\n",
      "Authenticating with existing credentials...\n",
      "Login Succeeded\n",
      "echo \"Building Docker image modeling:latest\"\n",
      "Building Docker image modeling:latest\n",
      "docker build --platform linux/amd64 -f infra/Dockerfile -t modeling:latest .\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)  docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)  docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (2/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.0s (4/16)                                   docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.1s\n",
      " => => transferring context: 4.43MB                                        0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (11/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.6s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.7s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.9s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.0s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.1s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             0.8s\n",
      "\u001b[2m => => # Processing /tmp/SPCSDataScience                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.2s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.0s\n",
      "\u001b[2m => => # Processing /tmp/SPCSDataScience                                       \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.4s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.1s\n",
      "\u001b[2m => => # Processing /tmp/SPCSDataScience                                       \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): finished with status 'done'          \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.5s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.3s\n",
      "\u001b[2m => => # Processing /tmp/SPCSDataScience                                       \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: SPCSDataScience               \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for SPCSDataScience (setup.py): started              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.4s\n",
      "\u001b[2m => => # Processing /tmp/SPCSDataScience                                       \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: SPCSDataScience               \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for SPCSDataScience (setup.py): started              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.4s\n",
      "\u001b[2m => => #   Preparing metadata (setup.py): started                              \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (setup.py): finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: SPCSDataScience               \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for SPCSDataScience (setup.py): started              \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for SPCSDataScience (setup.py): finished with status \n",
      "\u001b[0m\u001b[2m => => # 'done'                                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.9s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.6s\n",
      "\u001b[2m => => #   Created wheel for SPCSDataScience: filename=SPCSDataScience-0.0.1-py\n",
      "\u001b[0m\u001b[2m => => # 3-none-any.whl size=15721 sha256=21cb7447c9340acfd6bbf812ffa3e27e78973\n",
      "\u001b[0m\u001b[2m => => # 7edc7c151fac0468ae55e12e841                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-4jew8fo0/wheels/39/2\n",
      "\u001b[0m\u001b[2m => => # 6/96/07816529f8574432f180c40e9de661d4d2e1f931e78c76f01a               \n",
      "\u001b[0m\u001b[2m => => # Successfully built SPCSDataScience                                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.0s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.7s\n",
      "\u001b[2m => => #   Created wheel for SPCSDataScience: filename=SPCSDataScience-0.0.1-py\n",
      "\u001b[0m\u001b[2m => => # 3-none-any.whl size=15721 sha256=21cb7447c9340acfd6bbf812ffa3e27e78973\n",
      "\u001b[0m\u001b[2m => => # 7edc7c151fac0468ae55e12e841                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-4jew8fo0/wheels/39/2\n",
      "\u001b[0m\u001b[2m => => # 6/96/07816529f8574432f180c40e9de661d4d2e1f931e78c76f01a               \n",
      "\u001b[0m\u001b[2m => => # Successfully built SPCSDataScience                                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.2s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             1.9s\n",
      "\u001b[2m => => #   Created wheel for SPCSDataScience: filename=SPCSDataScience-0.0.1-py\n",
      "\u001b[0m\u001b[2m => => # 3-none-any.whl size=15721 sha256=21cb7447c9340acfd6bbf812ffa3e27e78973\n",
      "\u001b[0m\u001b[2m => => # 7edc7c151fac0468ae55e12e841                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-4jew8fo0/wheels/39/2\n",
      "\u001b[0m\u001b[2m => => # 6/96/07816529f8574432f180c40e9de661d4d2e1f931e78c76f01a               \n",
      "\u001b[0m\u001b[2m => => # Successfully built SPCSDataScience                                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.3s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.0s\n",
      "\u001b[2m => => #   Created wheel for SPCSDataScience: filename=SPCSDataScience-0.0.1-py\n",
      "\u001b[0m\u001b[2m => => # 3-none-any.whl size=15721 sha256=21cb7447c9340acfd6bbf812ffa3e27e78973\n",
      "\u001b[0m\u001b[2m => => # 7edc7c151fac0468ae55e12e841                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-4jew8fo0/wheels/39/2\n",
      "\u001b[0m\u001b[2m => => # 6/96/07816529f8574432f180c40e9de661d4d2e1f931e78c76f01a               \n",
      "\u001b[0m\u001b[2m => => # Successfully built SPCSDataScience                                    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.4s (12/16)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.1s\n",
      "\u001b[2m => => # 3-none-any.whl size=15721 sha256=21cb7447c9340acfd6bbf812ffa3e27e78973\n",
      "\u001b[0m\u001b[2m => => # 7edc7c151fac0468ae55e12e841                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /tmp/pip-ephem-wheel-cache-4jew8fo0/wheels/39/2\n",
      "\u001b[0m\u001b[2m => => # 6/96/07816529f8574432f180c40e9de661d4d2e1f931e78c76f01a               \n",
      "\u001b[0m\u001b[2m => => # Successfully built SPCSDataScience                                    \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: SPCSDataScience                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.6s (13/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.2s\n",
      "\u001b[0m => [10/12] RUN useradd -m jupyter                                         0.2s\n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "\u001b[4A\u001b[0G\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.6s (14/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN useradd -m jupyter                                         0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.8s (15/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN useradd -m jupyter                                         0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] WORKDIR /home/jupyter                                          0.0s\n",
      "\u001b[0m => [12/12] RUN chown -R jupyter:jupyter /home/jupyter                     0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.9s (16/16)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN useradd -m jupyter                                         0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] WORKDIR /home/jupyter                                          0.0s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN chown -R jupyter:jupyter /home/jupyter                     0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.0s (17/17) FINISHED                         docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.53kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/intel/intel-optimized-ml:lates  1.9s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/intel/intel-optimized-ml:latest@sha256:657a78e  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/intel/intel-optimized-ml:latest@sha256:657a78ebe  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.2s\n",
      "\u001b[0m\u001b[34m => => transferring context: 17.22MB                                       0.2s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN apt-get update && apt-get install -y python3-venv   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN curl -fsSL https://cli.github.com/packages/githubc  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/12] RUN echo \"deb [arch=$(dpkg --print-architecture) signe  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/12] RUN apt-get update && apt-get install -y gh             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/12] COPY infra/requirements.txt /tmp/requirements.txt       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/12] RUN python3 -m venv /opt/venv &&     /opt/venv/bin/pip  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/12] COPY ../. /tmp/SPCSDataScience                                 0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN /opt/venv/bin/pip install /tmp/SPCSDataScience             2.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN useradd -m jupyter                                         0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] WORKDIR /home/jupyter                                          0.0s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN chown -R jupyter:jupyter /home/jupyter                     0.2s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.1s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:a16ce9b12ce94493bfbc617195165b1680ed9648d6e4c  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/modeling:latest                         0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "View build details: \u001b]8;;docker-desktop://dashboard/build/desktop-linux/desktop-linux/00est4s41lb785rry3e2vzv49\u001b\\docker-desktop://dashboard/build/desktop-linux/desktop-linux/00est4s41lb785rry3e2vzv49\u001b]8;;\u001b\\\n",
      "\n",
      "Build multi-platform images faster with Docker Build Cloud: \u001b]8;;https://docs.docker.com/go/docker-build-cloud\u001b\\https://docs.docker.com/go/docker-build-cloud\u001b]8;;\u001b\\\n",
      "echo \"Pushing Docker image modeling:latest to sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\"\n",
      "Pushing Docker image modeling:latest to sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo\n",
      "docker tag modeling:latest sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo/modeling:latest\n",
      "docker push sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo/modeling:latest\n",
      "The push refers to repository [sfsenorthamerica-demo-jdemlow.registry.snowflakecomputing.com/container_demo_db/public/image_repo/modeling]\n",
      "\n",
      "\u001b[1Bf35efb06: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B85a58165: Preparing \n",
      "\u001b[1Bc5af1d5e: Preparing \n",
      "\u001b[1B0a17bf5b: Preparing \n",
      "\u001b[1B9fc5cb3d: Preparing \n",
      "\u001b[1B3728caed: Preparing \n",
      "\u001b[1B606e79c4: Preparing \n",
      "\u001b[1B62491732: Preparing \n",
      "\u001b[1Bb075bf0d: Preparing \n",
      "\u001b[1B7cca00b2: Preparing \n",
      "\u001b[1B92bc3378: Preparing \n",
      "\u001b[1B9da5597d: Preparing \n",
      "\u001b[1B929b1066: Preparing \n",
      "\u001b[1B6b9f6a79: Preparing \n",
      "\u001b[1B4a16fb11: Preparing \n",
      "\u001b[1B443ed903: Preparing \n",
      "\u001b[17Bf18a086: Preparing \n",
      "\u001b[1B97fe0755: Preparing \n",
      "\u001b[1B932c6beb: Preparing \n",
      "\u001b[16Bfc5cb3d: Pushed   1.352GB/1.33GBB\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[21A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[10A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[8A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[8A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[11A\u001b[2K\u001b[3A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[2A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2KPushing   1.25GB/1.33GB\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2Klatest: digest: sha256:52128b4f7c93ad4ea633f552beb9a295f67c8df3b3fb1f044691feea9497b0e1 size: 4930\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "! cd ../ &&  make all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note title=\"Explain Make File Call\" collapse=\"true\"}\n",
    "The `make all` command in the context of a Makefile is a target that typically encompasses a series of dependent tasks required to build a project. It's designed to automate the process of compiling, building, and preparing a software application or service. Here's a step-by-step explanation of what `make all` does in the provided Makefile script:\n",
    "\n",
    "1. **`login:`** This target logs into the Snowflake Docker repository. It's the initial step to ensure that subsequent operations, such as pushing a Docker image, can authenticate against the Snowflake registry. The `docker login` command uses the `$(SNOWFLAKE_REPO)` variable, which should contain the URL of the Snowflake Docker repository.\n",
    "\n",
    "2. **`build:`** This target builds a Docker image for Snowpark Container Services. It specifies the platform as `linux/amd64`, uses a Dockerfile located in the `infra` directory, and tags the resulting image with the name specified in the `$(DOCKER_IMAGE)` variable. This step prepares the Docker image with the necessary environment and dependencies for the application.\n",
    "\n",
    "3. **`push_docker:`** After the Docker image is built, this target tags and pushes it to the Snowflake Container Services repository specified in the `$(SNOWFLAKE_REPO)` variable. This makes the Docker image available in Snowflake's registry, allowing it to be used in Snowflake Container Services.\n",
    "\n",
    "In summary, `make all` in this script is a composite command that automates the workflow of logging into the Snowflake Docker repository, building a Docker image tailored for Snowpark Container Services, and pushing the built image to the Snowflake repository. This streamlines the deployment process, ensuring that the Docker image is readily available in Snowflake for running services or applications.\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output: false\n",
    "execute_sql_file(\n",
    "    session,\n",
    "    file_path='DataScience/files/sql/01_container_services.sql'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note title=\"Explain Container Services Execution Code\" collapse=\"true\"}\n",
    "\n",
    "# Snowflake Configuration for Enhanced Security and Services\n",
    "\n",
    "This documentation guides you through the process of configuring Snowflake for enhanced security measures and service creation, specifically tailored for modeling and development purposes within containerized environments.\n",
    "\n",
    "## Step 1: Create Security Integration for OAuth\n",
    "\n",
    "The process begins with the creation of an OAuth security integration. This is a critical step for setting up authentication mechanisms, ensuring secure access to Snowflake services.\n",
    "\n",
    "```sql\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE SECURITY INTEGRATION IF NOT EXISTS snowservices_ingress_oauth\n",
    "  TYPE=oauth\n",
    "  OAUTH_CLIENT=snowservices_ingress\n",
    "  ENABLED=true;\n",
    "```\n",
    "\n",
    "## Step 2: Configure Network Rules and External Access\n",
    "\n",
    "Following the security setup, network rules are defined to allow outbound traffic, facilitating external communications. An external access integration is also created to authorize this specified traffic, bolstering the security framework.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE NETWORK RULE ALLOW_ALL_RULE\n",
    "  TYPE = 'HOST_PORT'\n",
    "  MODE = 'EGRESS'\n",
    "  VALUE_LIST= ('0.0.0.0:443', '0.0.0.0:80');\n",
    "\n",
    "CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION ALLOW_ALL_EAI\n",
    "  ALLOWED_NETWORK_RULES = (ALLOW_ALL_RULE)\n",
    "  ENABLED = true;\n",
    "```\n",
    "\n",
    "## Step 3: Grant Usage on Integrations\n",
    "\n",
    "Permissions are then granted to the `CONTAINER_USER_ROLE` for the external access integration, ensuring the role has the necessary access to utilize the integration for service communications.\n",
    "\n",
    "```sql\n",
    "GRANT USAGE ON INTEGRATION ALLOW_ALL_EAI TO ROLE CONTAINER_USER_ROLE;\n",
    "```\n",
    "\n",
    "## Step 4: Create and Modify Compute Pool\n",
    "\n",
    "A compute pool is established with specific node limits and an instance family, aligning with processing needs. This step also includes provisions for modifying the compute pool's configuration to adjust resources as needed.\n",
    "\n",
    "```sql\n",
    "CREATE COMPUTE POOL IF NOT EXISTS CONTAINER_DEMO_POOL\n",
    "  MIN_NODES = 1\n",
    "  MAX_NODES = 2\n",
    "  INSTANCE_FAMILY = standard_1;\n",
    "GRANT USAGE ON COMPUTE POOL CONTAINER_DEMO_POOL TO ROLE CONTAINER_USER_ROLE;\n",
    "```\n",
    "\n",
    "## Step 5: Create Modeling Snowpark Service\n",
    "\n",
    "Next, a Snowpark service named `MODELING_SNOWPARK_SERVICE` is created using the specified compute pool. This service is designed for modeling purposes, incorporating external access integrations and specification details for optimal functionality.\n",
    "\n",
    "```sql\n",
    "USE ROLE CONTAINER_USER_ROLE;\n",
    "CREATE SERVICE CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE\n",
    "in compute pool CONTAINER_DEMO_POOL\n",
    "from @SPECS\n",
    "specification_file='modeling.yaml'\n",
    "external_access_integrations = (ALLOW_ALL_EAI)\n",
    "MIN_INSTANCES=1\n",
    "MAX_INSTANCES=1;\n",
    "```\n",
    "\n",
    "## Step 6: User Creation and Role Assignment\n",
    "\n",
    "Finally, a user is created with specified credentials, default role, and warehouse settings. The `CONTAINER_USER_ROLE` is granted to the user, along with usage permissions on various resources to ensure comprehensive access and operational capabilities.\n",
    "\n",
    "```sql\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE OR REPLACE USER RANDOMEMPLOYEE\n",
    "IDENTIFIED BY 'Snowflake2024'\n",
    "DEFAULT_ROLE = 'CONTAINER_USER_ROLE'\n",
    "DEFAULT_WAREHOUSE = 'CONTAINER_DEMO_WH';\n",
    "GRANT ROLE CONTAINER_USER_ROLE TO USER RANDOMEMPLOYEE;\n",
    "GRANT USAGE ON WAREHOUSE CONTAINER_DEMO_WH TO ROLE CONTAINER_USER_ROLE;\n",
    "GRANT USAGE ON SERVICE CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE TO ROLE CONTAINER_USER_ROLE;\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look Into Service Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"name\"                     |\"database_name\"    |\"schema_name\"  |\"owner\"              |\"compute_pool\"       |\"dns_name\"                                          |\"min_instances\"  |\"max_instances\"  |\"auto_resume\"  |\"external_access_integrations\"  |\"created_on\"                      |\"updated_on\"                      |\"resumed_on\"  |\"comment\"  |\"owner_role_type\"  |\"query_warehouse\"  |\"is_job\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|MODELING_SNOWPARK_SERVICE  |CONTAINER_DEMO_DB  |PUBLIC         |CONTAINER_USER_ROLE  |CONTAINER_DEMO_POOL  |modeling-snowpark-service.public.container-demo...  |1                |1                |true           |[\"ALLOW_ALL_EAI\"]               |2024-04-25 09:35:40.550000-07:00  |2024-04-25 09:35:41.823000-07:00  |NULL          |NULL       |ROLE               |NULL               |false     |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"SHOW SERVICES;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: That this will take a few minutes of your service to be up and running so you will be able run the following command again and it will give you the url link that will allow you to log into your app with the correct Username and Password.\n",
    "\n",
    "> ``CALL SYSTEM$GET_SERVICE_STATUS('CONTAINER_DEMO_DB.PUBLIC.MODELING_SNOWPARK_SERVICE');`` You can also try this command to see the status of your service you might get an ingress_url before the pending state is resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>ingress_enabled</th>\n",
       "      <th>ingress_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeling</td>\n",
       "      <td>8080</td>\n",
       "      <td>HTTP</td>\n",
       "      <td>true</td>\n",
       "      <td>bg54b6u-sfsenorthamerica-demo-jdemlow.snowflak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  port protocol ingress_enabled  \\\n",
       "0  modeling  8080     HTTP            true   \n",
       "\n",
       "                                         ingress_url  \n",
       "0  bg54b6u-sfsenorthamerica-demo-jdemlow.snowflak...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg54b6u-sfsenorthamerica-demo-jdemlow.snowflakecomputing.app\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "temp = pd.DataFrame(session.sql(\"SHOW ENDPOINTS IN SERVICE MODELING_SNOWPARK_SERVICE;\").collect())\n",
    "display(temp)\n",
    "print(temp[\"ingress_url\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming ingest_url contains the URL you want to copy and paste this URL into your browser and enjoy your modeling jupyter notebook.\n",
    "\n",
    "1. Open in web broswer and paste the ingress_url\n",
    "2. This will open a login page for this demo it is\n",
    "    - Usename: RandomEmployee\n",
    "    - Password: Snowflake2024\n",
    "3. Development App Will Load and Your Application is ready to go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake Application Deployment\n",
    "\n",
    "## Overview\n",
    "This guide will walk you through the process of deploying a Snowflake application, cloning a repository, setting up GitHub secrets, triggering a GitHub Actions workflow, and exploring the resulting models and predictions stored in Snowflake.\n",
    "\n",
    "### Prerequisites\n",
    "Before you begin, ensure that you have the following:\n",
    "- Docker installed and configured on your local machine\n",
    "- A GitHub account with a repository set up for your project\n",
    "- Access to a Snowflake instance with the necessary permissions\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "With the Docker image now deployed and ready, we are poised to explore the capabilities of our newly created application. Navigate to a website with a URL ending in `.snowflakecomputing.app` to begin.\n",
    "\n",
    "![Open Application](./images/openapp.png)\n",
    "\n",
    "Upon reaching the site, log in using the credentials for \"RandomEmployee\". This process grants access to your development ecosystem.\n",
    "\n",
    "> **Warning**: Allow 5-10 minutes for the application to become fully operational. Initial errors can often be resolved by refreshing the page or attempting to log in again after a brief wait.\n",
    "\n",
    "The next step involves using a secure token found within your Docker Image. If unchanged, the default token \"my_secure_token\" enhances application security, although its use is optional.\n",
    "\n",
    "![First Screen Add Token](./images/login_first_page.png)\n",
    "\n",
    "Post-login, you may notice the development ecosystem appears empty. This is due to the absence of a cloned repository. Although it may seem unconventional, cloning a repo into the development application addresses a known product gap, set for future resolution. Additionally, integration with Git in PrPr is forthcoming, offering a more streamlined experience with repositories.\n",
    "\n",
    "![Open Terminal as Jupyter Lab Is Empty](./images/open_terminal.png)\n",
    "\n",
    "![Clone Repo to Jupyter Lab Session](./images/clone_repo.png)\n",
    "\n",
    "This will prompt your user name and password you can use a personal token to allow for yourself to load in your repo, but there are many other ways, but to keep it simple in this tutorial this was a good way to get this done quickly.\n",
    "\n",
    "```bash\n",
    "gh auth login\n",
    "git config --global user.name \"sfc-gsfc-gh-jdemlow\"\n",
    "git config --global credential.helper cacheche\n",
    "```\n",
    "\n",
    "Having cloned your repository, feel free to start development. Remember to push changes back to your Git repository to remain updated with the latest revisions. Your code is safely stored in a designated volume, accessible via `ls @volumes/jupyter-nbs/` in Snowflake, complying with specified requirements.\n",
    "\n",
    "## GitHub Integration\n",
    "\n",
    "To integrate your project with GitHub and enable automated workflows, follow these steps:\n",
    "\n",
    "1. **Set up GitHub Secrets**: Ensure your GitHub repository contains the necessary secrets for authentication and access to your Snowflake instance. The setup should resemble the image below.\n",
    "\n",
    "   ![Add GitHub Secrets](./images/add_secerts.png)\n",
    "\n",
    "   Refer to the [GitHub Secrets documentation](https://docs.github.com/en/actions/security-guides/encrypted-secrets) for detailed instructions on how to create and manage secrets for your repository.\n",
    "\n",
    "2. **Configure GitHub Actions**: With GitHub secrets in place, navigate to your repository's GitHub Actions to trigger the workflow.\n",
    "\n",
    "   ![Go to Repo Actions](./images/github_action.png)\n",
    "\n",
    "3. **Trigger the Workflow**: Manually trigger the workflow by selecting the appropriate action and providing any necessary inputs.\n",
    "\n",
    "   ![Trigger Workflow](./images/run_github_action.png)\n",
    "\n",
    "   Successfully triggering the workflow culminates in the construction of a RandomForestRegressor. This model, once trained, is utilized for predictions, subsequently recorded back into Snowflake. Future tutorials will delve into more sophisticated workflows, enhancing your data science projects within Snowflake.\n",
    "\n",
    "> **Caution**: This tutorial does not encompass the entirety of a data science project's requirements. It serves as an introductory guide, with plans to expand on Snowflake's native ML capabilities, Cortex, and other projects. Stay tuned for updates and explore different branches for additional developments.\n",
    "\n",
    "## Exploring Results\n",
    "\n",
    "After workflow completion, delve into the notebooks for insights and outcomes verification.\n",
    "\n",
    "![Click into job_nbs](./images/click_jobs_nbs.png)\n",
    "\n",
    "![Click into RanJobs](./images/jobs_ran_from_github.png)\n",
    "\n",
    "![Result of GitHub Actions](./images/result_of_action.png)\n",
    "\n",
    "Reviewing these notebooks aids in debugging and ensures everything performed as expected.\n",
    "\n",
    "![Explore Inference Notebook Run](./images/look_at_ran_nbs.png)\n",
    "\n",
    "For continued exploration, re-triggering the notebook facilitates additional predictions, exemplifying how automation, such as cron jobs in GitHub, can enhance your workflow.\n",
    "\n",
    "![Run Action Again](./images/run_again.png)\n",
    "\n",
    "Finally, view the prediction outcomes within your Snowflake database.\n",
    "\n",
    "![Snowflake Database](./images/final_result.png)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter any issues during the deployment process, here are some common troubleshooting steps:\n",
    "\n",
    "1. **Login Issues**: If you're unable to log in, ensure that you're using the correct credentials and that the application has had sufficient time to become operational.\n",
    "2. **GitHub Secrets**: Double-check that your GitHub secrets are set up correctly and that the required permissions are in place.\n",
    "3. **Workflow Errors**: Review the output from the GitHub Actions workflow for any error messages or clues about what might be causing the issue.\n",
    "4. **Snowflake Connectivity**: Verify that your Snowflake instance is accessible and that you have the necessary permissions to interact with it.\n",
    "\n",
    "If the issue persists, feel free to reach out to the support team for further assistance.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "For more advanced topics, code examples, and updates, please refer to the following resources:\n",
    "\n",
    "- [Snowflake Documentation](https://docs.snowflake.com/)\n",
    "- [Snowflake GitHub Repository](https://github.com/snowflakedb)\n",
    "- [Snowflake Community](https://community.snowflake.com/)\n",
    "- [GitHub Actions Documentation](https://docs.github.com/en/actions)\n",
    "\n",
    "We encourage you to provide feedback on this documentation by submitting issues or pull requests.\n",
    "\n",
    "\n",
    "> WARNING: This is intended for development purposes only and has not been approved by security or infosec for deployment. If you plan to deploy this, be aware of the risks, as it has not been fully secured. Please consult with your data governance teams to ensure safe deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output: false\n",
    "execute_sql_file(\n",
    "    session,\n",
    "    file_path='DataScience/files/sql/03_teardown.sql'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just as simple example of the power of creating modeling application with in snowflake in a fast an secure fashion that you are able to share with your teams across your organizations please make sure you come back for more examples like this as we will begin to build more and more use cases inside of the SPCS as this is going to be a powerful building block to unlock the full potential of snowflake as a platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
